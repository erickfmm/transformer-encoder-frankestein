graph TB
    subgraph "Input Layer"
        A[Input IDs] --> B[Word Embeddings<br/>Linear: vocab_size→1024]
        B --> C[Dropout 0.1]
    end
    
    subgraph "Recursive Loop x2 (Logical Depth = 24)"
        C --> D[Loop Start]
        
        subgraph "Hybrid Layer x12 (Pattern: retnet→ode→mamba→titan_attn)"
            D --> E[Dynamic Tanh Norm]
            
            subgraph "Mixer Block"
                E --> F{Layer Type<br/>layer_idx % 4}
                
                F -->|0: RetNet| G1[Multi-Scale Retention<br/>BitLinear QKV + Decay]
                F -->|1: ODE| G2[Neural ODE Attention<br/>RK4 Solver + BitLinear]
                F -->|2: Mamba| G3[Mamba SSM<br/>BitLinear Placeholder]
                F -->|3: Titan Attn| G4[Standard Attention<br/>BitLinear QKV]
                
                G1 --> H[Mixer Output]
                G2 --> H
                G3 --> H
                G4 --> H
            end
            
            H --> I[Residual + Dropout]
            I --> J[Dynamic Tanh Norm]
            
            subgraph "Sparse MoE Block (BitNet)"
                J --> K[Router: Linear<br/>1024→4<br/>no bias]
                K --> L[Top-K Selection<br/>k=2 experts]
                
                subgraph "Expert Processing"
                    L --> M[Expert 1: BitLinear<br/>1024→2048→1024 + SiLU]
                    L --> N[Expert 2: BitLinear<br/>1024→2048→1024 + SiLU]
                    L --> O[Expert 3: BitLinear<br/>1024→2048→1024 + SiLU]
                    L --> P[Expert 4: BitLinear<br/>1024→2048→1024 + SiLU]
                end
                
                M --> Q[Weighted Expert Sum]
                N --> Q
                O --> Q
                P --> Q
            end
            
            Q --> R[Residual + Dropout]
            R --> S[To Next Layer]
        end
        
        S --> T[Next Loop Iteration]
        T --> D
    end
    
    subgraph "Output Layer"
        S --> U[Final Dynamic Tanh Norm]
        U --> V[BitLinear Head<br/>1024→vocab_size]
        V --> W[Output Logits]
    end
    
    subgraph "Training & Loss"
        W --> X[MLM Loss<br/>CrossEntropyLoss]
        X --> Y[AdamW Optimizer<br/>Component-specific LR]
        Y --> Z[Cosine LR Schedule<br/>with Warmup]
    end
    
    %% Styling
    style G1 fill:#e0f2f1
    style G2 fill:#e8eaf6
    style G3 fill:#fff3e0
    style G4 fill:#fce4ec
    style K fill:#ffe1e1
    style M fill:#fff3e0
    style N fill:#fff3e0
    style O fill:#fff3e0
    style P fill:#fff3e0
    style V fill:#fff3e0
    style E fill:#e1bee7
    style J fill:#e1bee7
    style U fill:#e1bee7