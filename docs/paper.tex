\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{array}
\usepackage[numbers]{natbib}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{geometry}
\geometry{margin=1in}

\title{Transformer Encoder Frankenstein: Library, CLI, and Research Grounded Design Notes}
\author{
Erick F. Merino M. \\
This work is not affiliated \\
\texttt{erickfmm@gmail.com}
}
\date{February 2026}

\begin{document}

\maketitle

\begin{abstract}
This document reframes Transformer Encoder Frankenstein as a configuration-driven software toolkit and research workbench. We present the CLI-first workflow, mathematical descriptions of supported attention and sequence-mixer families, optimizer families mapped to the training schema, deployment quantization mechanics, and SBERT downstream tasks. We also include pseudocode and summary tables to make design trade-offs explicit for implementation and experimentation.
\end{abstract}

\section{Introduction}

Transformer systems are now a production concern as much as a modeling concern \citep{vaswani_attention_2017,devlin_bert_2019}. In practice, users need a single toolchain that can: (i) define training configurations with strict contracts, (ii) train with multiple optimizer and mixer families, (iii) deploy quantized checkpoints, and (iv) run sentence-embedding workflows inspired by SBERT \citep{reimers_sentence-bert_2019}.

The project command surface is:
\begin{center}
\texttt{frankestein-transformer}
\end{center}
with subcommands:
\texttt{train}, \texttt{deploy}, \texttt{quantize}, \texttt{infer}, \texttt{sbert-train}, \texttt{sbert-infer}.

\section{Configuration-Centric Architecture}

The authoritative contract is \texttt{src/training/configs/schema.yaml}. It enforces three top-level objects:
\begin{itemize}[leftmargin=*]
    \item \texttt{model\_class}
    \item \texttt{model}
    \item \texttt{training}
\end{itemize}

The \texttt{model.layer\_pattern} supports:
\[
\{\texttt{retnet}, \texttt{mamba}, \texttt{ode}, \texttt{titan\_attn}, \texttt{standard\_attn}, \texttt{sigmoid\_attn}\}
\]
which corresponds to current attention and sequence-mixer literature \citep{sun_retentive_2023,gu_mamba_2023,zhang_continuous_2021,behrouz_titans_2025,ramapuram_theory_2024,vaswani_attention_2017}.

The \texttt{training.optimizer.optimizer\_class} supports a broad optimizer family:
\texttt{adamw}, \texttt{adafactor}, \texttt{radam}, \texttt{adan}, \texttt{adopt}, \texttt{ademamix}, \texttt{mars\_adamw}, \texttt{cautious\_adamw}, \texttt{schedulefree\_adamw}, \texttt{lion}, \texttt{sophia}, \texttt{prodigy}, \texttt{muon}, \texttt{turbo\_muon}, \texttt{shampoo}, \texttt{soap}, and others.

\section{Attention and Sequence-Mixer Families}

\subsection{Standard Attention}
Given projected matrices $(Q,K,V)$:
\[
\text{Attn}(Q,K,V)=\text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
\]
This is the baseline mechanism for content routing \citep{vaswani_attention_2017}.

\subsection{Sigmoid Attention}
Sigmoid attention removes row-wise probability normalization:
\[
\text{SigmoidAttn}(Q,K,V)=\sigma\left(\frac{QK^\top}{\sqrt{d_k}}+b\right)V
\]
and has different training stability requirements, often with additional normalization \citep{ramapuram_theory_2024}.

\subsection{Retentive Formulation}
RetNet uses retention with decay matrix $D$:
\[
\text{Retention}(Q,K,V)=\left(QK^\top \odot D\right)V
\]
with recurrent form:
\[
S_n=\gamma S_{n-1}+k_n^\top v_n,\qquad o_n=q_n S_n
\]
enabling low-cost recurrent inference \citep{sun_retentive_2023,yang_survey_2025}.

\subsection{Selective SSM (Mamba)}
Discrete selective state-space recurrence is:
\[
h_t=\bar{A}_t h_{t-1}+\bar{B}_t x_t,\qquad y_t=C_t h_t
\]
where $(\bar{A}_t,\bar{B}_t,C_t)$ depend on input, preserving linear-time scaling with hardware-aware scan \citep{gu_mamba_2023,hwang_hydra_2024}.

\subsection{ODE-style Continuous Updates}
Continuous-depth framing:
\[
\frac{dh(t)}{dt}=f_\theta(h(t),t)
\]
with practical RK integrators for discrete execution \citep{zhang_continuous_2021}.

\subsection{Test-time Memory (Titans)}
A memory-augmented update can be written:
\[
M_t=(1-\alpha_t)M_{t-1}+S_t,\qquad
S_t=\eta_t S_{t-1}-\theta_t\nabla \ell(M_{t-1};x_t)
\]
to adapt memory at inference time \citep{behrouz_titans_2025,behrouz_titans_2024}.

\begin{algorithm}[t]
\caption{Pattern-Driven Mixer Forward (Conceptual)}
\begin{algorithmic}[1]
\Require Hidden states $H$, pattern $P$, layer index $\ell$
\State $m \gets P[\ell \bmod |P|]$
\If{$m=\texttt{standard\_attn}$}
    \State $H \gets \text{softmax-attention}(H)$
\ElsIf{$m=\texttt{sigmoid\_attn}$}
    \State $H \gets \text{sigmoid-attention}(H)$
\ElsIf{$m=\texttt{retnet}$}
    \State $H \gets \text{retention}(H)$
\ElsIf{$m=\texttt{mamba}$}
    \State $H \gets \text{selective-ssm}(H)$
\ElsIf{$m=\texttt{ode}$}
    \State $H \gets \text{rk-step}(H)$
\Else
    \State $H \gets \text{memory-augmented-attn}(H)$
\EndIf
\State \Return $H$
\end{algorithmic}
\end{algorithm}

\section{Optimizer Families and Training Dynamics}

\subsection{Core Adaptive Form}
Many supported optimizers share moment tracking:
\[
m_t=\beta_1 m_{t-1} + (1-\beta_1)g_t,\quad
v_t=\beta_2 v_{t-1} + (1-\beta_2)g_t^2
\]
followed by preconditioned updates (e.g., AdamW) \citep{loshchilov_decoupled_2017}.

\subsection{Examples from the Supported Set}
\begin{itemize}[leftmargin=*]
    \item \textbf{RAdam}: variance rectification for early-step instability \citep{liu_variance_2019}.
    \item \textbf{Adan}: adaptive Nesterov momentum for faster convergence \citep{xie_adan_2022}.
    \item \textbf{ADOPT}: modified Adam order yielding stronger convergence guarantees \citep{taniguchi_adopt_2024}.
    \item \textbf{AdEMAMix}: dual-EMA history mixing \citep{pagliardini_ademamix_2024}.
    \item \textbf{MARS}: variance reduction in preconditioned optimization \citep{yuan_mars_2024}.
    \item \textbf{Cautious optimizers}: sign-consistent masking of momentum updates \citep{liang_cautious_2024}.
    \item \textbf{Schedule-free}: remove explicit scheduler dependence \citep{defazio_road_2024}.
    \item \textbf{Shampoo/SOAP}: matrix preconditioning families \citep{gupta_shampoo_2018,vyas_soap_2024}.
    \item \textbf{Adafactor/GaLore}: memory reduction via factorization or low-rank projection \citep{shazeer_adafactor_2018,zhao_galore_2024}.
    \item \textbf{Prodigy/Lion/Sophia}: parameter-free adaptation, sign momentum, and clipped second-order scaling \citep{mishchenko_prodigy_2023,chen_symbolic_2023,liu_sophia_2023}.
    \item \textbf{Muon/Turbo-Muon}: orthogonality-oriented updates with acceleration \citep{shen_convergence_2025,boissin_turbo-muon_2025}.
\end{itemize}

\begin{algorithm}[t]
\caption{Schema-Routed Optimizer Step (Conceptual)}
\begin{algorithmic}[1]
\Require Parameters $\theta$, gradients $g$, optimizer class $c$, parameter map $\Pi$
\State Read optimizer-specific hyperparameters from prefixed keys in $\Pi$
\If{$c=\texttt{adamw}$} \State apply AdamW step \citep{loshchilov_decoupled_2017}
\ElsIf{$c=\texttt{radam}$} \State apply rectified adaptive step \citep{liu_variance_2019}
\ElsIf{$c=\texttt{adan}$} \State apply Adan three-moment step \citep{xie_adan_2022}
\ElsIf{$c=\texttt{adopt}$} \State apply ADOPT update ordering \citep{taniguchi_adopt_2024}
\ElsIf{$c=\texttt{galore\_adamw}$} \State project gradients to low-rank subspace then step \citep{zhao_galore_2024}
\Else \State dispatch to selected optimizer implementation
\EndIf
\State \Return updated $\theta$
\end{algorithmic}
\end{algorithm}

\section{Quantization and Deployment}

The deploy stack uses ternary weight packing plus INT8 activation quantization for efficient artifacts.

\subsection{Ternary Quantization}
Given weight tensor $W$, a practical scaling is:
\[
s=\text{mean}(|W|),\qquad
\tilde{W}=\text{clip}\left(\text{round}\left(\frac{W}{s}\right),-1,1\right)
\]
which approximates BitNet-style low-bit updates \citep{wang_bitnet_2023}. The packed mapping uses two bits per weight symbol for storage efficiency.

\subsection{Activation Quantization}
For activations $x$:
\[
q=\text{round}\left(x\cdot\frac{127}{\max(|x|)+\epsilon}\right),\qquad q\in[-128,127]
\]
with dequantization $x\approx q/\alpha$.

\subsection{Size Estimates}
For $N$ parameters:
\[
\text{FP32 size}\approx 4N,\quad \text{FP16 size}\approx 2N,\quad \text{1.58-bit size}\approx \frac{1.58}{8}N
\]
before metadata and packing overhead. This aligns with lightweight deployment goals \citep{samson_lightweight_2026,bravin_embbert_2026}.

\section{SBERT Downstream Tasks}

Sentence embedding is built on Siamese-style training \citep{reimers_sentence-bert_2019}. For sentence pair $(s_1,s_2)$ with embeddings $(e_1,e_2)$:
\[
\text{cos}(e_1,e_2)=\frac{e_1^\top e_2}{\|e_1\|\|e_2\|}
\]
and regression-style cosine loss:
\[
\mathcal{L}_{\text{cos}}=\left(\text{cos}(e_1,e_2)-y\right)^2
\]
with $y\in[-1,1]$ in this pipeline.

Supported downstream modes:
\begin{itemize}[leftmargin=*]
    \item \textbf{Similarity}: pairwise score between two sentences.
    \item \textbf{Search}: top-$k$ nearest neighbors over a corpus.
    \item \textbf{Cluster}: grouping embeddings (e.g., k-means).
    \item \textbf{Encode}: persistent embedding export for later retrieval.
\end{itemize}

\begin{algorithm}[t]
\caption{SBERT Inference Mode Router}
\begin{algorithmic}[1]
\Require mode $m$, model $E$, inputs $X$
\If{$m=\texttt{similarity}$}
    \State return $\cos(E(x_1),E(x_2))$
\ElsIf{$m=\texttt{search}$}
    \State return top-$k$ by dot-product/cosine against corpus embeddings
\ElsIf{$m=\texttt{cluster}$}
    \State return clustering labels over $E(X)$
\Else
    \State return serialized embeddings $E(X)$
\EndIf
\end{algorithmic}
\end{algorithm}

\section{Summary Tables}

\subsection{Attention and Sequence-Mixer Summary}
\small
\begin{longtable}{p{2.4cm}p{3.2cm}p{2.0cm}p{2.0cm}p{4.8cm}}
\toprule
\textbf{Type} & \textbf{Core Equation} & \textbf{Train} & \textbf{Infer} & \textbf{Notes} \\
\midrule
\endhead
Standard Attention & $\text{softmax}(QK^\top/\sqrt{d_k})V$ & $\mathcal{O}(n^2d)$ & $\mathcal{O}(n)$/step & Baseline expressive global routing \citep{vaswani_attention_2017}. \\
Sigmoid Attention & $\sigma(QK^\top/\sqrt{d_k}+b)V$ & $\mathcal{O}(n^2d)$ & $\mathcal{O}(n)$/step & Element-wise gating; often needs stabilization norm \citep{ramapuram_theory_2024}. \\
RetNet & $(QK^\top\odot D)V$ & $\mathcal{O}(n^2d)$ or chunkwise & $\mathcal{O}(1)$/step & Parallel/recurrent dual form with decay retention \citep{sun_retentive_2023}. \\
Mamba & $h_t=\bar{A}_t h_{t-1}+\bar{B}_t x_t$ & $\mathcal{O}(nd)$ & $\mathcal{O}(1)$/step & Selective state-space with hardware-aware scan \citep{gu_mamba_2023}. \\
ODE-style block & $\frac{dh}{dt}=f_\theta(h,t)$ & solver-dependent & solver-dependent & Continuous-depth interpretation; RK integration \citep{zhang_continuous_2021}. \\
Titans memory & $M_t=(1-\alpha_t)M_{t-1}+S_t$ & approx.\ $\mathcal{O}(nd)$ & retrieval-centric & Test-time memory updates with surprise-driven dynamics \citep{behrouz_titans_2025}. \\
\bottomrule
\end{longtable}
\normalsize

\subsection{Optimizer Summary}
\small
\begin{longtable}{p{2.2cm}p{2.8cm}p{2.0cm}p{5.4cm}p{1.8cm}}
\toprule
\textbf{Optimizer} & \textbf{Family} & \textbf{State Cost} & \textbf{Key Idea} & \textbf{Ref} \\
\midrule
\endhead
AdamW & Adaptive first/second moment & High & Decoupled weight decay baseline & \citep{loshchilov_decoupled_2017} \\
RAdam & Adaptive variance-corrected & High & Rectifies early adaptive variance & \citep{liu_variance_2019} \\
Adan & Momentum + variance reduction & High & Nesterov-style adaptive update & \citep{xie_adan_2022} \\
ADOPT & Adam variant & High & Reordered updates with improved convergence guarantees & \citep{taniguchi_adopt_2024} \\
AdEMAMix & Multi-EMA adaptive & High & Mixes short and long horizon EMAs & \citep{pagliardini_ademamix_2024} \\
MARS & Variance-reduced preconditioned & High & Recursive momentum correction & \citep{yuan_mars_2024} \\
Cautious AdamW & Masked momentum & High & Apply updates only on sign-consistent directions & \citep{liang_cautious_2024} \\
Schedule-free AdamW & Scheduler-free adaptive & High & Remove explicit LR schedule dependence & \citep{defazio_road_2024} \\
Adafactor & Memory-efficient adaptive & Medium & Factorized second moments for matrix tensors & \citep{shazeer_adafactor_2018} \\
GaLore AdamW & Low-rank gradient projection & Medium & Optimize in projected low-rank gradient space & \citep{zhao_galore_2024} \\
Prodigy & Parameter-free adaptation & Medium & Distance-adaptive step calibration & \citep{mishchenko_prodigy_2023} \\
Lion & Sign momentum & Low & Momentum sign update, reduced state & \citep{chen_symbolic_2023} \\
Sophia & Approx.\ second-order & Medium & Diagonal Hessian preconditioning with clipping & \citep{liu_sophia_2023} \\
Shampoo & Matrix preconditioner & High & Kronecker-structured second-order statistics & \citep{gupta_shampoo_2018} \\
SOAP & Shampoo + Adam basis & High & Adam-like tracking in preconditioner eigenbasis & \citep{vyas_soap_2024} \\
Muon & Orthogonality-based & Medium & Orthogonalized matrix updates & \citep{shen_convergence_2025} \\
Turbo-Muon & Accelerated orthogonalization & Medium & Preconditioned Newton-Schulz speedup & \citep{boissin_turbo-muon_2025} \\
\bottomrule
\end{longtable}
\normalsize

\section{Discussion}

From an engineering perspective, the toolkit couples modern research ideas with reproducible interfaces:
\begin{itemize}[leftmargin=*]
    \item Explicit schema contracts lower configuration ambiguity.
    \item Multiple attention/mixer families let users tune for context length, latency, and memory.
    \item Broad optimizer support enables controlled studies over convergence and stability.
    \item Quantized deployment reduces artifact size and improves portability.
    \item SBERT workflows cover practical retrieval and semantic similarity tasks.
\end{itemize}

The design also aligns with multilingual and compact-model directions in the literature \citep{canete_spanish_2023,sun_mobilebert_2020,bui_knowledge_2024,gillin_bert-jepa_2026}.

\section{Conclusion}

Transformer Encoder Frankenstein is positioned as a practical experimentation platform: a strict configuration schema, extensible optimizer and attention families, deploy-time quantization, and sentence embedding workflows in one CLI. This makes it suitable for both rapid iteration and reproducible model operations.

% Include all requested bibliography collections.
\nocite{zhang_continuous_2021,sun_mobilebert_2020,yang_looped_2024,reimers_sentence-bert_2019,wang_bitnet_2023,devlin_bert_2019,canete_spanish_2023,cai_survey_2025,yang_survey_2025,behrouz_titans_2024,gillin_bert-jepa_2026,bui_knowledge_2024,samson_lightweight_2026,zhu_transformers_2025,chen_stronger_2025,yongueng_holonorm_2025,bravin_embbert_2026,hwang_hydra_2024,dai_hope_2025,sun_retentive_2023,gu_mamba_2024,zhang_why_2020}
\nocite{loshchilov_decoupled_2017,liu_variance_2019,xie_adan_2022,taniguchi_adopt_2024,pagliardini_ademamix_2024,yuan_mars_2024,liang_cautious_2024,defazio_road_2024,gupta_shampoo_2018,vyas_soap_2024,shazeer_adafactor_2018,zhao_galore_2024,mishchenko_prodigy_2023,chen_symbolic_2023,liu_sophia_2023,shen_convergence_2025,boissin_turbo-muon_2025}
\nocite{behrouz_titans_2025,gu_mamba_2023,ramapuram_theory_2024,vaswani_attention_2017}

\bibliographystyle{plainnat}
\bibliography{bibliography/other,bibliography/optimizers,bibliography/attention_types}

\end{document}
