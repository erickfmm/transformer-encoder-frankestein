\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{array}
\usepackage[numbers]{natbib}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{geometry}
\geometry{margin=1in}

\title{Transformer Encoder Frankenstein: Library, CLI, and Research Grounded Design Notes}
\author{
Erick F. Merino M. \\
This work is not affiliated \\
\texttt{erickfmm@gmail.com}
}
\date{February 2026}

\begin{document}

\maketitle

\begin{abstract}
This document reframes Transformer Encoder Frankenstein as a configuration-driven software toolkit and research workbench. We present the CLI-first workflow, mathematical descriptions of supported attention and sequence-mixer families, optimizer families mapped to the training schema, deployment quantization mechanics, and SBERT downstream tasks. We also include pseudocode and summary tables to make design trade-offs explicit for implementation and experimentation.
\end{abstract}

\section{Introduction}

Transformer systems are now a production concern as much as a modeling concern \citep{vaswani_attention_2017,devlin_bert_2019}. In practice, users need a single toolchain that can: (i) define training configurations with strict contracts, (ii) train with multiple optimizer and mixer families, (iii) deploy quantized checkpoints, and (iv) run sentence-embedding workflows inspired by SBERT \citep{reimers_sentence-bert_2019}.

The project command surface is:
\begin{center}
\texttt{frankestein-transformer}
\end{center}
with subcommands:
\texttt{train}, \texttt{deploy}, \texttt{quantize}, \texttt{infer}, \texttt{sbert-train}, \texttt{sbert-infer}.

\section{Configuration-Centric Architecture}

The authoritative contract is \texttt{src/training/configs/schema.yaml}. It enforces three top-level objects:
\begin{itemize}[leftmargin=*]
    \item \texttt{model\_class}
    \item \texttt{model}
    \item \texttt{training}
\end{itemize}

The \texttt{model.layer\_pattern} supports:
\[
\{\texttt{retnet}, \texttt{mamba}, \texttt{ode}, \texttt{titan\_attn}, \texttt{standard\_attn}, \texttt{sigmoid\_attn}\}
\]
which corresponds to current attention and sequence-mixer literature \citep{sun_retentive_2023,gu_mamba_2023,zhang_continuous_2021,behrouz_titans_2025,ramapuram_theory_2024,vaswani_attention_2017}.

The \texttt{training.optimizer.optimizer\_class} supports a broad optimizer family:
\texttt{adamw}, \texttt{adafactor}, \texttt{radam}, \texttt{adan}, \texttt{adopt}, \texttt{ademamix}, \texttt{mars\_adamw}, \texttt{cautious\_adamw}, \texttt{schedulefree\_adamw}, \texttt{lion}, \texttt{sophia}, \texttt{prodigy}, \texttt{muon}, \texttt{turbo\_muon}, \texttt{shampoo}, \texttt{soap}, and others.

\subsection{Schema Scope and Validation Rules}

The schema is strict: top-level and nested objects set \texttt{additionalProperties: false}. This guarantees that unknown keys fail fast instead of being silently ignored. The \texttt{training.optimizer.parameters} object is additionally constrained by optimizer-specific prefix rules through \texttt{allOf}+\texttt{if/then} pattern checks.

Normalization values currently accepted by schema are:
\[
\texttt{norm\_type} \in \{\texttt{layer\_norm}, \texttt{dynamic\_tanh}, \texttt{derf}\}
\]
Thus, \texttt{rms\_norm} is \textbf{not} a valid schema value in the current contract.

\subsection{Complete Model Feature Inventory}

\small
\begin{longtable}{p{3.1cm}p{2.4cm}p{7.0cm}}
\toprule
\textbf{Field} & \textbf{Type/Range} & \textbf{Meaning} \\
\midrule
\endhead
\texttt{vocab\_size} & int $\ge 1$ & Vocabulary size. \\
\texttt{hidden\_size} & int $\ge 1$ & Hidden dimension. \\
\texttt{num\_layers} & int $\ge 1$ & Physical layer count. \\
\texttt{num\_loops} & int $\ge 1$ & Logical loop count (looped blocks). \\
\texttt{num\_heads} & int $\ge 1$ & Attention heads. \\
\texttt{retention\_heads} & int $\ge 1$ & Retention heads for RetNet-style mixers. \\
\texttt{num\_experts} & int $\ge 1$ & MoE expert count. \\
\texttt{top\_k\_experts} & int $\ge 1$ & Top-$k$ expert routing in MoE. \\
\texttt{dropout} & float $[0,1]$ & Global dropout. \\
\texttt{layer\_pattern} & array enum & Ordered block list: \texttt{retnet}, \texttt{mamba}, \texttt{ode}, \texttt{titan\_attn}, \texttt{standard\_attn}, \texttt{sigmoid\_attn}. \\
\texttt{ode\_solver} & enum & \texttt{rk4} or \texttt{euler}. \\
\texttt{ode\_steps} & int $\ge 1$ & ODE integration steps. \\
\texttt{use\_bitnet} & bool & Enable low-bit BitLinear path. \\
\texttt{norm\_type} & enum & \texttt{layer\_norm}, \texttt{dynamic\_tanh}, \texttt{derf}. \\
\texttt{use\_factorized\_embedding} & bool & Enable factorized embeddings. \\
\texttt{factorized\_embedding\_dim} & int $\ge 1$ & Reduced embedding dimension for factorization. \\
\texttt{use\_embedding\_conv} & bool & Enable Conv1d over embedding stream. \\
\texttt{embedding\_conv\_kernel} & int $\ge 1$ & Conv1d kernel size. \\
\texttt{hope\_base} & float $\ge 0$ & HoPE base value (optional in schema). \\
\texttt{hope\_damping} & float $\ge 0$ & HoPE damping (optional in schema). \\
\texttt{use\_hope} & bool & Apply HoPE in \texttt{titan\_attn}. \\
\texttt{use\_moe} & bool & Enable MoE FFN routing path. \\
\texttt{ffn\_hidden\_size} & int $\ge 1$ & FFN intermediate width. \\
\texttt{ffn\_activation} & enum & \texttt{silu} or \texttt{gelu}. \\
\bottomrule
\end{longtable}
\normalsize

Looped depth induced by schema is:
\[
L_{\text{logical}} = \texttt{num\_layers} \times \texttt{num\_loops}
\]
which is the configuration-level definition of looped blocks.

\subsection{Complete Training Feature Inventory}

\small
\begin{longtable}{p{3.1cm}p{2.4cm}p{7.0cm}}
\toprule
\textbf{Field} & \textbf{Type/Range} & \textbf{Meaning} \\
\midrule
\endhead
\texttt{batch\_size} & int $\ge 1$ & Loader batch size. \\
\texttt{dataloader\_workers} & int $\ge 0$ & PyTorch dataloader workers. \\
\texttt{max\_length} & int $\ge 1$ & Sequence length cap. \\
\texttt{mlm\_probability} & float $[0,1]$ & MLM masking probability. \\
\texttt{max\_samples} & int $\ge 1$ & Maximum streamed samples. \\
\texttt{dataset\_batch\_size} & int $\ge 1$ & Internal streaming dataset chunk size. \\
\texttt{num\_workers} & int $\ge 0$ & Streaming dataset workers. \\
\texttt{cache\_dir} & string & Dataset cache directory. \\
\texttt{local\_parquet\_dir} & string & Optional local parquet path. \\
\texttt{prefer\_local\_cache} & bool & Prefer local cache when available. \\
\texttt{stream\_local\_parquet} & bool & Stream from local parquet mode. \\
\texttt{use\_amp} & bool & Mixed precision toggle. \\
\texttt{gradient\_accumulation\_steps} & int $\ge 1$ & Effective batch through accumulation. \\
\texttt{optimizer} & object & Contains \texttt{optimizer\_class} and prefixed \texttt{parameters}. \\
\texttt{scheduler\_total\_steps} & int $\ge 1$ & Scheduler horizon. \\
\texttt{scheduler\_warmup\_ratio} & float $[0,1]$ & Warmup ratio. \\
\texttt{scheduler\_type} & enum & \texttt{cosine}, \texttt{constant}, \texttt{linear\_warmup\_then\_constant}. \\
\texttt{grad\_clip\_max\_norm} & float $\ge 0$ & Global norm clipping threshold. \\
\texttt{inf\_post\_clip\_threshold} & float $\ge 0$ & Exploding-gradient guard threshold after clipping. \\
\texttt{max\_nan\_retries} & int $\ge 0$ & Retry budget for NaN/Inf instability. \\
\texttt{checkpoint\_every\_n\_steps} & int $\ge 1$ & Rolling checkpoint frequency. \\
\texttt{max\_rolling\_checkpoints} & int $\ge 1$ & Number of rolling checkpoints to keep. \\
\texttt{num\_best\_checkpoints} & int $\ge 1$ & Number of best checkpoints tracked. \\
\texttt{nan\_check\_interval} & int $\ge 1$ & NaN/Inf check cadence. \\
\texttt{log\_gradient\_stats} & bool & Enable gradient statistics logging. \\
\texttt{gradient\_log\_interval} & int $\ge 1$ & Gradient logging cadence. \\
\texttt{csv\_log\_path} & string & Step-level CSV output path. \\
\texttt{csv\_rotate\_on\_schema\_change} & bool & Rotate CSV if logging schema changes. \\
\texttt{gpu\_metrics\_backend} & enum & \texttt{nvml} or \texttt{none}. \\
\texttt{nvml\_device\_index} & int $\ge 0$ & Device index for NVML telemetry. \\
\texttt{enable\_block\_grad\_norms} & bool & Include per-block gradient norm telemetry. \\
\texttt{telemetry\_log\_interval} & int $\ge 1$ & Heavy telemetry interval (optimizer steps). \\
\texttt{use\_galore} & bool & Enable GaLore strategy. \\
\texttt{galore\_rank} & int $\ge 1$ & GaLore low-rank projection dimension. \\
\texttt{galore\_update\_interval} & int $\ge 1$ & Projection refresh interval. \\
\texttt{galore\_scale} & float $\ge 0$ & Gradient scaling in projected space. \\
\texttt{galore\_max\_dim} & int $\ge 1$ & Maximum tensor dimension for GaLore projection. \\
\bottomrule
\end{longtable}
\normalsize

\subsection{Optimizer Prefix Contract (Full)}

Supported \texttt{optimizer\_class} values are:
\texttt{sgd\_momentum}, \texttt{adamw}, \texttt{adafactor}, \texttt{galore\_adamw}, \texttt{prodigy}, \texttt{lion}, \texttt{sophia}, \texttt{muon}, \texttt{turbo\_muon}, \texttt{radam}, \texttt{adan}, \texttt{adopt}, \texttt{ademamix}, \texttt{mars\_adamw}, \texttt{cautious\_adamw}, \texttt{lamb}, \texttt{schedulefree\_adamw}, \texttt{shampoo}, \texttt{soap}.

Shared per-group suffix families (all prefixed by optimizer name) are:
\begin{itemize}[leftmargin=*]
    \item LR groups: \texttt{lr\_embeddings}, \texttt{lr\_norms}, \texttt{lr\_ode}, \texttt{lr\_retnet}, \texttt{lr\_mamba}, \texttt{lr\_attention}, \texttt{lr\_other}
    \item Weight decay groups: \texttt{wd\_embeddings}, \texttt{wd\_norms}, \texttt{wd\_ode}, \texttt{wd\_retnet}, \texttt{wd\_mamba}, \texttt{wd\_attention}, \texttt{wd\_other}
    \item Beta groups: \texttt{betas\_embeddings}, \texttt{betas\_norms}, \texttt{betas\_ode}, \texttt{betas\_retnet}, \texttt{betas\_mamba}, \texttt{betas\_attention}, \texttt{betas\_other}
    \item Epsilon groups: \texttt{eps\_embeddings}, \texttt{eps\_norms}, \texttt{eps\_ode}, \texttt{eps\_retnet}, \texttt{eps\_mamba}, \texttt{eps\_attention}, \texttt{eps\_other}
\end{itemize}

Optimizer-specific global suffixes:
\begin{itemize}[leftmargin=*]
    \item \texttt{sgd\_momentum}: \texttt{momentum}, \texttt{nesterov}
    \item \texttt{adafactor}: \texttt{beta2\_decay}, \texttt{clip\_threshold}, \texttt{eps1}, \texttt{eps2}
    \item \texttt{galore\_adamw}: \texttt{rank}, \texttt{update\_proj\_gap}
    \item \texttt{prodigy}: \texttt{d\_coef}
    \item \texttt{sophia}: \texttt{rho}, \texttt{update\_k}
    \item \texttt{muon} / \texttt{turbo\_muon}: \texttt{momentum}, \texttt{nesterov}, \texttt{ns\_steps}, \texttt{ns\_eps}
    \item \texttt{cautious\_adamw}: \texttt{cautious\_clip}
\end{itemize}
All other classes in the list above accept only prefixed shared groups.

\subsection{Training Safety and Runtime Semantics}

Schema-level safety features include accumulation, clipping, post-clip explosion checks, and NaN retries:
\[
g_{\text{acc}}=\frac{1}{K}\sum_{i=1}^{K} g_i,\quad K=\texttt{gradient\_accumulation\_steps}
\]
\[
g_{\text{clip}} = g_{\text{acc}}\cdot
\min\left(1,\frac{\tau}{\|g_{\text{acc}}\|_2+\epsilon}\right),\quad
\tau=\texttt{grad\_clip\_max\_norm}
\]
then overflow guards use \texttt{inf\_post\_clip\_threshold} and retry logic bounded by \texttt{max\_nan\_retries}.

\begin{algorithm}[t]
\caption{Schema-Driven Training Step with Stability Controls}
\begin{algorithmic}[1]
\Require Batch stream, config $C$
\State Initialize retry counter $r\gets 0$
\For{each optimizer step}
    \State Accumulate gradients for $K=C.\texttt{gradient\_accumulation\_steps}$ micro-batches
    \State Apply global norm clipping with $\tau=C.\texttt{grad\_clip\_max\_norm}$
    \If{post-clip gradient exceeds $C.\texttt{inf\_post\_clip\_threshold}$ or NaN/Inf detected}
        \If{$r < C.\texttt{max\_nan\_retries}$}
            \State restore safe state / skip step; $r \gets r+1$
            \State \textbf{continue}
        \Else
            \State stop training with failure state
        \EndIf
    \EndIf
    \State run optimizer step selected by \texttt{optimizer\_class}
    \State update scheduler (\texttt{cosine}, \texttt{constant}, or \texttt{linear\_warmup\_then\_constant})
    \If{step mod \texttt{checkpoint\_every\_n\_steps}$=0$}
        \State save rolling checkpoint and prune to \texttt{max\_rolling\_checkpoints}
    \EndIf
    \State update best checkpoints up to \texttt{num\_best\_checkpoints}
    \State emit CSV + telemetry following \texttt{gradient\_log\_interval} and \texttt{telemetry\_log\_interval}
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Normalization Variants: RMSNorm, Dynamic Tanh, and Dynamic Erf}

Normalization and normalization-alternatives are central to training stability and throughput in transformer-like systems. Based on ArXiv sources, the three relevant formulations are:

\subsection{RMSNorm}
RMSNorm removes mean-centering and only rescales by root mean square magnitude \citep{zhang_root_2019}:
\[
\mathrm{RMS}(x)=\sqrt{\frac{1}{d}\sum_{i=1}^{d}x_i^2+\epsilon},\qquad
y_i=\gamma_i\frac{x_i}{\mathrm{RMS}(x)}
\]
Compared with LayerNorm, RMSNorm is computationally simpler (no subtraction of feature mean) and is often used when reducing normalization overhead is important.

\subsection{Dynamic Tanh (DyT)}
Dynamic Tanh proposes replacing explicit normalization with a bounded elementwise map \citep{zhu_transformers_2025}:
\[
\mathrm{DyT}(x)=\tanh(\alpha x)
\]
where $\alpha$ is learned. The core idea is that bounded nonlinear contraction can provide stable signal scaling without explicitly computing per-token normalization statistics.

\subsection{Dynamic Erf (Derf)}
Derf extends the same normalization-free direction by using an error-function based map \citep{chen_stronger_2025}:
\[
\mathrm{Derf}(x)=\mathrm{erf}(\alpha x+s)
\]
with learnable scale/shift. Reported results in the cited work indicate stronger performance than DyT and common normalization baselines across multiple domains.

\subsection{Schema Implications}
Current configuration contract in this repository allows:
\[
\texttt{norm\_type}\in\{\texttt{layer\_norm},\texttt{dynamic\_tanh},\texttt{derf}\}
\]
so DyT and Derf are directly available in schema-driven runs, while RMSNorm is not currently an accepted enum value and would require code/schema extension.

\small
\begin{longtable}{p{2.6cm}p{3.8cm}p{2.6cm}p{4.4cm}}
\toprule
\textbf{Method} & \textbf{Formula} & \textbf{Stats Needed} & \textbf{Notes} \\
\midrule
\endhead
RMSNorm & $y_i=\gamma_i x_i/\sqrt{\frac{1}{d}\sum_j x_j^2+\epsilon}$ & RMS only & Lower overhead than LayerNorm; widely used normalization baseline \citep{zhang_root_2019}. \\
Dynamic Tanh & $\tanh(\alpha x)$ & none & Normalization-free bounded transform; simple drop-in replacement direction \citep{zhu_transformers_2025}. \\
Dynamic Erf (Derf) & $\mathrm{erf}(\alpha x+s)$ & none & Normalization-free alternative designed to improve over DyT \citep{chen_stronger_2025}. \\
\bottomrule
\end{longtable}
\normalsize

\section{Attention and Sequence-Mixer Families}

\subsection{Standard Attention}
Given projected matrices $(Q,K,V)$:
\[
\text{Attn}(Q,K,V)=\text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
\]
This is the baseline mechanism for content routing \citep{vaswani_attention_2017}.

\subsection{Sigmoid Attention}
Sigmoid attention removes row-wise probability normalization:
\[
\text{SigmoidAttn}(Q,K,V)=\sigma\left(\frac{QK^\top}{\sqrt{d_k}}+b\right)V
\]
and has different training stability requirements, often with additional normalization \citep{ramapuram_theory_2024}.

\subsection{Retentive Formulation}
RetNet uses retention with decay matrix $D$:
\[
\text{Retention}(Q,K,V)=\left(QK^\top \odot D\right)V
\]
with recurrent form:
\[
S_n=\gamma S_{n-1}+k_n^\top v_n,\qquad o_n=q_n S_n
\]
enabling low-cost recurrent inference \citep{sun_retentive_2023,yang_survey_2025}.

\subsection{Selective SSM (Mamba)}
Discrete selective state-space recurrence is:
\[
h_t=\bar{A}_t h_{t-1}+\bar{B}_t x_t,\qquad y_t=C_t h_t
\]
where $(\bar{A}_t,\bar{B}_t,C_t)$ depend on input, preserving linear-time scaling with hardware-aware scan \citep{gu_mamba_2023,hwang_hydra_2024}.

\subsection{ODE-style Continuous Updates}
Continuous-depth framing:
\[
\frac{dh(t)}{dt}=f_\theta(h(t),t)
\]
with practical RK integrators for discrete execution \citep{zhang_continuous_2021}.

\subsection{Test-time Memory (Titans)}
A memory-augmented update can be written:
\[
M_t=(1-\alpha_t)M_{t-1}+S_t,\qquad
S_t=\eta_t S_{t-1}-\theta_t\nabla \ell(M_{t-1};x_t)
\]
to adapt memory at inference time \citep{behrouz_titans_2025,behrouz_titans_2024}.

\begin{algorithm}[t]
\caption{Pattern-Driven Mixer Forward (Conceptual)}
\begin{algorithmic}[1]
\Require Hidden states $H$, pattern $P$, layer index $\ell$
\State $m \gets P[\ell \bmod |P|]$
\If{$m=\texttt{standard\_attn}$}
    \State $H \gets \text{softmax-attention}(H)$
\ElsIf{$m=\texttt{sigmoid\_attn}$}
    \State $H \gets \text{sigmoid-attention}(H)$
\ElsIf{$m=\texttt{retnet}$}
    \State $H \gets \text{retention}(H)$
\ElsIf{$m=\texttt{mamba}$}
    \State $H \gets \text{selective-ssm}(H)$
\ElsIf{$m=\texttt{ode}$}
    \State $H \gets \text{rk-step}(H)$
\Else
    \State $H \gets \text{memory-augmented-attn}(H)$
\EndIf
\State \Return $H$
\end{algorithmic}
\end{algorithm}

\section{Optimizer Families and Training Dynamics}

\subsection{Core Adaptive Form}
Many supported optimizers share moment tracking:
\[
m_t=\beta_1 m_{t-1} + (1-\beta_1)g_t,\quad
v_t=\beta_2 v_{t-1} + (1-\beta_2)g_t^2
\]
followed by preconditioned updates (e.g., AdamW) \citep{loshchilov_decoupled_2017}.

\subsection{Examples from the Supported Set}
\begin{itemize}[leftmargin=*]
    \item \textbf{RAdam}: variance rectification for early-step instability \citep{liu_variance_2019}.
    \item \textbf{Adan}: adaptive Nesterov momentum for faster convergence \citep{xie_adan_2022}.
    \item \textbf{ADOPT}: modified Adam order yielding stronger convergence guarantees \citep{taniguchi_adopt_2024}.
    \item \textbf{AdEMAMix}: dual-EMA history mixing \citep{pagliardini_ademamix_2024}.
    \item \textbf{MARS}: variance reduction in preconditioned optimization \citep{yuan_mars_2024}.
    \item \textbf{Cautious optimizers}: sign-consistent masking of momentum updates \citep{liang_cautious_2024}.
    \item \textbf{Schedule-free}: remove explicit scheduler dependence \citep{defazio_road_2024}.
    \item \textbf{Shampoo/SOAP}: matrix preconditioning families \citep{gupta_shampoo_2018,vyas_soap_2024}.
    \item \textbf{Adafactor/GaLore}: memory reduction via factorization or low-rank projection \citep{shazeer_adafactor_2018,zhao_galore_2024}.
    \item \textbf{Prodigy/Lion/Sophia}: parameter-free adaptation, sign momentum, and clipped second-order scaling \citep{mishchenko_prodigy_2023,chen_symbolic_2023,liu_sophia_2023}.
    \item \textbf{Muon/Turbo-Muon}: orthogonality-oriented updates with acceleration \citep{shen_convergence_2025,boissin_turbo-muon_2025}.
\end{itemize}

\begin{algorithm}[t]
\caption{Schema-Routed Optimizer Step (Conceptual)}
\begin{algorithmic}[1]
\Require Parameters $\theta$, gradients $g$, optimizer class $c$, parameter map $\Pi$
\State Read optimizer-specific hyperparameters from prefixed keys in $\Pi$
\If{$c=\texttt{adamw}$} \State apply AdamW step \citep{loshchilov_decoupled_2017}
\ElsIf{$c=\texttt{radam}$} \State apply rectified adaptive step \citep{liu_variance_2019}
\ElsIf{$c=\texttt{adan}$} \State apply Adan three-moment step \citep{xie_adan_2022}
\ElsIf{$c=\texttt{adopt}$} \State apply ADOPT update ordering \citep{taniguchi_adopt_2024}
\ElsIf{$c=\texttt{galore\_adamw}$} \State project gradients to low-rank subspace then step \citep{zhao_galore_2024}
\Else \State dispatch to selected optimizer implementation
\EndIf
\State \Return updated $\theta$
\end{algorithmic}
\end{algorithm}

\section{Quantization and Deployment}

The deploy stack uses ternary weight packing plus INT8 activation quantization for efficient artifacts.

\subsection{Ternary Quantization}
Given weight tensor $W$, a practical scaling is:
\[
s=\text{mean}(|W|),\qquad
\tilde{W}=\text{clip}\left(\text{round}\left(\frac{W}{s}\right),-1,1\right)
\]
which approximates BitNet-style low-bit updates \citep{wang_bitnet_2023}. The packed mapping uses two bits per weight symbol for storage efficiency.

\subsection{Activation Quantization}
For activations $x$:
\[
q=\text{round}\left(x\cdot\frac{127}{\max(|x|)+\epsilon}\right),\qquad q\in[-128,127]
\]
with dequantization $x\approx q/\alpha$.

\subsection{Size Estimates}
For $N$ parameters:
\[
\text{FP32 size}\approx 4N,\quad \text{FP16 size}\approx 2N,\quad \text{1.58-bit size}\approx \frac{1.58}{8}N
\]
before metadata and packing overhead. This aligns with lightweight deployment goals \citep{samson_lightweight_2026,bravin_embbert_2026}.

\section{SBERT Downstream Tasks}

Sentence embedding is built on Siamese-style training \citep{reimers_sentence-bert_2019}. For sentence pair $(s_1,s_2)$ with embeddings $(e_1,e_2)$:
\[
\text{cos}(e_1,e_2)=\frac{e_1^\top e_2}{\|e_1\|\|e_2\|}
\]
and regression-style cosine loss:
\[
\mathcal{L}_{\text{cos}}=\left(\text{cos}(e_1,e_2)-y\right)^2
\]
with $y\in[-1,1]$ in this pipeline.

Supported downstream modes:
\begin{itemize}[leftmargin=*]
    \item \textbf{Similarity}: pairwise score between two sentences.
    \item \textbf{Search}: top-$k$ nearest neighbors over a corpus.
    \item \textbf{Cluster}: grouping embeddings (e.g., k-means).
    \item \textbf{Encode}: persistent embedding export for later retrieval.
\end{itemize}

\begin{algorithm}[t]
\caption{SBERT Inference Mode Router}
\begin{algorithmic}[1]
\Require mode $m$, model $E$, inputs $X$
\If{$m=\texttt{similarity}$}
    \State return $\cos(E(x_1),E(x_2))$
\ElsIf{$m=\texttt{search}$}
    \State return top-$k$ by dot-product/cosine against corpus embeddings
\ElsIf{$m=\texttt{cluster}$}
    \State return clustering labels over $E(X)$
\Else
    \State return serialized embeddings $E(X)$
\EndIf
\end{algorithmic}
\end{algorithm}

\section{Summary Tables}

\subsection{Attention and Sequence-Mixer Summary}
\small
\begin{longtable}{p{2.4cm}p{3.2cm}p{2.0cm}p{2.0cm}p{4.8cm}}
\toprule
\textbf{Type} & \textbf{Core Equation} & \textbf{Train} & \textbf{Infer} & \textbf{Notes} \\
\midrule
\endhead
Standard Attention & $\text{softmax}(QK^\top/\sqrt{d_k})V$ & $\mathcal{O}(n^2d)$ & $\mathcal{O}(n)$/step & Baseline expressive global routing \citep{vaswani_attention_2017}. \\
Sigmoid Attention & $\sigma(QK^\top/\sqrt{d_k}+b)V$ & $\mathcal{O}(n^2d)$ & $\mathcal{O}(n)$/step & Element-wise gating; often needs stabilization norm \citep{ramapuram_theory_2024}. \\
RetNet & $(QK^\top\odot D)V$ & $\mathcal{O}(n^2d)$ or chunkwise & $\mathcal{O}(1)$/step & Parallel/recurrent dual form with decay retention \citep{sun_retentive_2023}. \\
Mamba & $h_t=\bar{A}_t h_{t-1}+\bar{B}_t x_t$ & $\mathcal{O}(nd)$ & $\mathcal{O}(1)$/step & Selective state-space with hardware-aware scan \citep{gu_mamba_2023}. \\
ODE-style block & $\frac{dh}{dt}=f_\theta(h,t)$ & solver-dependent & solver-dependent & Continuous-depth interpretation; RK integration \citep{zhang_continuous_2021}. \\
Titans memory & $M_t=(1-\alpha_t)M_{t-1}+S_t$ & approx.\ $\mathcal{O}(nd)$ & retrieval-centric & Test-time memory updates with surprise-driven dynamics \citep{behrouz_titans_2025}. \\
\bottomrule
\end{longtable}
\normalsize

\subsection{Optimizer Summary}
\small
\begin{longtable}{p{2.2cm}p{2.8cm}p{2.0cm}p{5.4cm}p{1.8cm}}
\toprule
\textbf{Optimizer} & \textbf{Family} & \textbf{State Cost} & \textbf{Key Idea} & \textbf{Ref} \\
\midrule
\endhead
AdamW & Adaptive first/second moment & High & Decoupled weight decay baseline & \citep{loshchilov_decoupled_2017} \\
RAdam & Adaptive variance-corrected & High & Rectifies early adaptive variance & \citep{liu_variance_2019} \\
Adan & Momentum + variance reduction & High & Nesterov-style adaptive update & \citep{xie_adan_2022} \\
ADOPT & Adam variant & High & Reordered updates with improved convergence guarantees & \citep{taniguchi_adopt_2024} \\
AdEMAMix & Multi-EMA adaptive & High & Mixes short and long horizon EMAs & \citep{pagliardini_ademamix_2024} \\
MARS & Variance-reduced preconditioned & High & Recursive momentum correction & \citep{yuan_mars_2024} \\
Cautious AdamW & Masked momentum & High & Apply updates only on sign-consistent directions & \citep{liang_cautious_2024} \\
Schedule-free AdamW & Scheduler-free adaptive & High & Remove explicit LR schedule dependence & \citep{defazio_road_2024} \\
Adafactor & Memory-efficient adaptive & Medium & Factorized second moments for matrix tensors & \citep{shazeer_adafactor_2018} \\
GaLore AdamW & Low-rank gradient projection & Medium & Optimize in projected low-rank gradient space & \citep{zhao_galore_2024} \\
Prodigy & Parameter-free adaptation & Medium & Distance-adaptive step calibration & \citep{mishchenko_prodigy_2023} \\
Lion & Sign momentum & Low & Momentum sign update, reduced state & \citep{chen_symbolic_2023} \\
Sophia & Approx.\ second-order & Medium & Diagonal Hessian preconditioning with clipping & \citep{liu_sophia_2023} \\
Shampoo & Matrix preconditioner & High & Kronecker-structured second-order statistics & \citep{gupta_shampoo_2018} \\
SOAP & Shampoo + Adam basis & High & Adam-like tracking in preconditioner eigenbasis & \citep{vyas_soap_2024} \\
Muon & Orthogonality-based & Medium & Orthogonalized matrix updates & \citep{shen_convergence_2025} \\
Turbo-Muon & Accelerated orthogonalization & Medium & Preconditioned Newton-Schulz speedup & \citep{boissin_turbo-muon_2025} \\
\bottomrule
\end{longtable}
\normalsize

\section{Discussion}

From an engineering perspective, the toolkit couples modern research ideas with reproducible interfaces:
\begin{itemize}[leftmargin=*]
    \item Explicit schema contracts lower configuration ambiguity.
    \item Multiple attention/mixer families let users tune for context length, latency, and memory.
    \item Broad optimizer support enables controlled studies over convergence and stability.
    \item Quantized deployment reduces artifact size and improves portability.
    \item SBERT workflows cover practical retrieval and semantic similarity tasks.
\end{itemize}

The design also aligns with multilingual and compact-model directions in the literature \citep{canete_spanish_2023,sun_mobilebert_2020,bui_knowledge_2024,gillin_bert-jepa_2026}.

\section{Conclusion}

Transformer Encoder Frankenstein is positioned as a practical experimentation platform: a strict configuration schema, extensible optimizer and attention families, deploy-time quantization, and sentence embedding workflows in one CLI. This makes it suitable for both rapid iteration and reproducible model operations.

% Include all requested bibliography collections.
\nocite{zhang_continuous_2021,sun_mobilebert_2020,yang_looped_2024,reimers_sentence-bert_2019,wang_bitnet_2023,devlin_bert_2019,canete_spanish_2023,cai_survey_2025,yang_survey_2025,behrouz_titans_2024,gillin_bert-jepa_2026,bui_knowledge_2024,samson_lightweight_2026,zhu_transformers_2025,chen_stronger_2025,yongueng_holonorm_2025,bravin_embbert_2026,hwang_hydra_2024,dai_hope_2025,sun_retentive_2023,gu_mamba_2024,zhang_why_2020}
\nocite{loshchilov_decoupled_2017,liu_variance_2019,xie_adan_2022,taniguchi_adopt_2024,pagliardini_ademamix_2024,yuan_mars_2024,liang_cautious_2024,defazio_road_2024,gupta_shampoo_2018,vyas_soap_2024,shazeer_adafactor_2018,zhao_galore_2024,mishchenko_prodigy_2023,chen_symbolic_2023,liu_sophia_2023,shen_convergence_2025,boissin_turbo-muon_2025}
\nocite{behrouz_titans_2025,gu_mamba_2023,ramapuram_theory_2024,vaswani_attention_2017}

\bibliographystyle{plainnat}
\bibliography{bibliography/other,bibliography/optimizers,bibliography/attention_types}

\end{document}
