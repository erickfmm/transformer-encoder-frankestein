
@misc{loshchilov_decoupled_2017,
	title = {Decoupled Weight Decay Regularization},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1711.05101},
	doi = {10.48550/ARXIV.1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard {SGD} and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with {SGD} with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in {TensorFlow} and {PyTorch}; the complete source code for our experiments is available at https://github.com/loshchil/{AdamW}-and-{SGDW}},
	publisher = {{arXiv}},
	author = {Loshchilov, Ilya and Hutter, Frank},
	urldate = {2026-02-24},
	date = {2017},
	note = {Version Number: 3},
	keywords = {{FOS}: Computer and information sciences, {FOS}: Mathematics, Machine Learning (cs.{LG}), Neural and Evolutionary Computing (cs.{NE}), Optimization and Control (math.{OC})},
}

@misc{liu_variance_2019,
	title = {On the Variance of the Adaptive Learning Rate and Beyond},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1908.03265},
	doi = {10.48550/ARXIV.1908.03265},
	abstract = {The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like {RMSprop} and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose {RAdam}, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: https://github.com/{LiyuanLucasLiu}/{RAdam}.},
	publisher = {{arXiv}},
	author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
	urldate = {2026-02-24},
	date = {2019},
	note = {Version Number: 4},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Machine Learning (stat.{ML})},
}

@misc{xie_adan_2022,
	title = {Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2208.06677},
	doi = {10.48550/ARXIV.2208.06677},
	shorttitle = {Adan},
	abstract = {In deep learning, different kinds of deep networks typically need different optimizers, which have to be chosen after multiple trials, making the training process inefficient. To relieve this issue and consistently improve the model training speed across deep networks, we propose the {ADAptive} Nesterov momentum algorithm, Adan for short. Adan first reformulates the vanilla Nesterov acceleration to develop a new Nesterov momentum estimation ({NME}) method, which avoids the extra overhead of computing gradient at the extrapolation point. Then, Adan adopts {NME} to estimate the gradient's first- and second-order moments in adaptive gradient algorithms for convergence acceleration. Besides, we prove that Adan finds an \$ε\$-approximate first-order stationary point within \${\textbackslash}mathcal\{O\}(ε{\textasciicircum}\{-3.5\})\$ stochastic gradient complexity on the non-convex stochastic problems (e.g., deep learning problems), matching the best-known lower bound. Extensive experimental results show that Adan consistently surpasses the corresponding {SoTA} optimizers on vision, language, and {RL} tasks and sets new {SoTAs} for many popular networks and frameworks, e.g., {ResNet}, {ConvNext}, {ViT}, Swin, {MAE}, {DETR}, {GPT}-2, Transformer-{XL}, and {BERT}. More surprisingly, Adan can use half of the training cost (epochs) of {SoTA} optimizers to achieve higher or comparable performance on {ViT}, {GPT}-2, {MAE}, etc., and also shows great tolerance to a large range of minibatch size, e.g., from 1k to 32k. Code is released at https://github.com/sail-sg/Adan, and has been used in multiple popular deep learning frameworks or projects.},
	publisher = {{arXiv}},
	author = {Xie, Xingyu and Zhou, Pan and Li, Huan and Lin, Zhouchen and Yan, Shuicheng},
	urldate = {2026-02-24},
	date = {2022},
	note = {Version Number: 5},
	keywords = {{FOS}: Computer and information sciences, {FOS}: Mathematics, Machine Learning (cs.{LG}), Optimization and Control (math.{OC})},
}

@misc{taniguchi_adopt_2024,
	title = {{ADOPT}: Modified Adam Can Converge with Any \$β\_2\$ with the Optimal Rate},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2411.02853},
	doi = {10.48550/ARXIV.2411.02853},
	shorttitle = {{ADOPT}},
	abstract = {Adam is one of the most popular optimization algorithms in deep learning. However, it is known that Adam does not converge in theory unless choosing a hyperparameter, i.e., \$β\_2\$, in a problem-dependent manner. There have been many attempts to fix the non-convergence (e.g., {AMSGrad}), but they require an impractical assumption that the gradient noise is uniformly bounded. In this paper, we propose a new adaptive gradient method named {ADOPT}, which achieves the optimal convergence rate of \${\textbackslash}mathcal\{O\} ( 1 / {\textbackslash}sqrt\{T\} )\$ with any choice of \$β\_2\$ without depending on the bounded noise assumption. {ADOPT} addresses the non-convergence issue of Adam by removing the current gradient from the second moment estimate and changing the order of the momentum update and the normalization by the second moment estimate. We also conduct intensive numerical experiments, and verify that our {ADOPT} achieves superior results compared to Adam and its variants across a wide range of tasks, including image classification, generative modeling, natural language processing, and deep reinforcement learning. The implementation is available at https://github.com/{iShohei}220/adopt.},
	publisher = {{arXiv}},
	author = {Taniguchi, Shohei and Harada, Keno and Minegishi, Gouki and Oshima, Yuta and Jeong, Seong Cheol and Nagahara, Go and Iiyama, Tomoshi and Suzuki, Masahiro and Iwasawa, Yusuke and Matsuo, Yutaka},
	urldate = {2026-02-24},
	date = {2024},
	note = {Version Number: 3},
	keywords = {{FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Machine Learning (stat.{ML})},
}

@misc{pagliardini_ademamix_2024,
	title = {The {AdEMAMix} Optimizer: Better, Faster, Older},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2409.03137},
	doi = {10.48550/ARXIV.2409.03137},
	shorttitle = {The {AdEMAMix} Optimizer},
	abstract = {Momentum based optimizers are central to a wide range of machine learning applications. These typically rely on an Exponential Moving Average ({EMA}) of gradients, which decays exponentially the present contribution of older gradients. This accounts for gradients being local linear approximations which lose their relevance as the iterate moves along the loss landscape. This work questions the use of a single {EMA} to accumulate past gradients and empirically demonstrates how this choice can be sub-optimal: a single {EMA} cannot simultaneously give a high weight to the immediate past, and a non-negligible weight to older gradients. Building on this observation, we propose {AdEMAMix}, a simple modification of the Adam optimizer with a mixture of two {EMAs} to better take advantage of past gradients. Our experiments on language modeling and image classification show -- quite surprisingly -- that gradients can stay relevant for tens of thousands of steps. They help to converge faster, and often to lower minima: e.g., a \$1.3\$B parameter {AdEMAMix} {LLM} trained on \$101\$B tokens performs comparably to an {AdamW} model trained on \$197\$B tokens (\$+95{\textbackslash}\%\$). Moreover, our method significantly slows-down model forgetting during training. Our work motivates further exploration of different types of functions to leverage past gradients, beyond {EMAs}.},
	publisher = {{arXiv}},
	author = {Pagliardini, Matteo and Ablin, Pierre and Grangier, David},
	urldate = {2026-02-24},
	date = {2024},
	note = {Version Number: 2},
	keywords = {{FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Machine Learning (stat.{ML})},
}

@misc{yuan_mars_2024,
	title = {{MARS}: Unleashing the Power of Variance Reduction for Training Large Models},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2411.10438},
	doi = {10.48550/ARXIV.2411.10438},
	shorttitle = {{MARS}},
	abstract = {Training deep neural networks--and more recently, large models demands efficient and scalable optimizers. Adaptive gradient algorithms like Adam, {AdamW}, and their variants have been central to this task. Despite the development of numerous variance reduction algorithms in the past decade aimed at accelerating stochastic optimization in both convex and nonconvex settings, variance reduction has not found widespread success in training deep neural networks or large language models. Consequently, it has remained a less favored approach in modern {AI}. In this paper, to unleash the power of variance reduction for efficient training of large models, we propose a unified optimization framework, {MARS} (Make {vAriance} Reduction Shine), which reconciles preconditioned gradient methods with variance reduction via a scaled stochastic recursive momentum technique. Within our framework, we introduce three instances of {MARS} that leverage preconditioned gradient updates based on {AdamW}, Lion, and Shampoo, respectively. We also draw a connection between our algorithms and existing optimizers. Experimental results on training {GPT}-2 models indicate that {MARS} consistently outperforms {AdamW} by a large margin. The implementation of {MARS} is available at https://github.com/{AGI}-Arena/{MARS}.},
	publisher = {{arXiv}},
	author = {Yuan, Huizhuo and Liu, Yifeng and Wu, Shuang and Zhou, Xun and Gu, Quanquan},
	urldate = {2026-02-24},
	date = {2024},
	note = {Version Number: 4},
	keywords = {{FOS}: Computer and information sciences, {FOS}: Mathematics, Machine Learning (cs.{LG}), Machine Learning (stat.{ML}), Optimization and Control (math.{OC})},
}

@misc{liang_cautious_2024,
	title = {Cautious Optimizers: Improving Training with One Line of Code},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2411.16085},
	doi = {10.48550/ARXIV.2411.16085},
	shorttitle = {Cautious Optimizers},
	abstract = {{AdamW} has been the default optimizer for transformer pretraining. For many years, our community searched for faster and more stable optimizers with only constrained positive outcomes. In this work, we propose a {\textbackslash}textbf\{one-line modification in Pytorch\} to any momentum-based optimizer, which we rename cautious optimizer, e.g. C-{AdamW} and C-Lion. Our theoretical result shows that this modification preserves Adam's Hamiltonian function and it does not break the convergence guarantee under the Lyapunov analysis. In addition, a whole new family of optimizers is revealed by our theoretical insight. Among them, we pick the simplest one for empirical experiments, showing not only consistent speed-up on {LLM} pretraining, but also image classification, with minimum extra tuning on hyperparameters. Code is available at https://github.com/kyleliang919/C-Optim.},
	publisher = {{arXiv}},
	author = {Liang, Kaizhao and Chen, Lizhang and Liu, Bo and Liu, Qiang},
	urldate = {2026-02-24},
	date = {2024},
	note = {Version Number: 4},
	keywords = {Artificial Intelligence (cs.{AI}), Computation and Language (cs.{CL}), Computer Vision and Pattern Recognition (cs.{CV}), Discrete Mathematics (cs.{DM}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
}

@misc{defazio_road_2024,
	title = {The Road Less Scheduled},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2405.15682},
	doi = {10.48550/ARXIV.2405.15682},
	abstract = {Existing learning rate schedules that do not require specification of the optimization stopping step T are greatly out-performed by learning rate schedules that depend on T. We propose an approach that avoids the need for this stopping time by eschewing the use of schedules entirely, while exhibiting state-of-the-art performance compared to schedules across a wide family of problems ranging from convex problems to large-scale deep learning problems. Our Schedule-Free approach introduces no additional hyper-parameters over standard optimizers with momentum. Our method is a direct consequence of a new theory we develop that unifies scheduling and iterate averaging. An open source implementation of our method is available at https://github.com/facebookresearch/schedule\_free. Schedule-Free {AdamW} is the core algorithm behind our winning entry to the {MLCommons} 2024 {AlgoPerf} Algorithmic Efficiency Challenge Self-Tuning track.},
	publisher = {{arXiv}},
	author = {Defazio, Aaron and Yang, Xingyu Alice and Mehta, Harsh and Mishchenko, Konstantin and Khaled, Ahmed and Cutkosky, Ashok},
	urldate = {2026-02-24},
	date = {2024},
	note = {Version Number: 4},
	keywords = {Artificial Intelligence (cs.{AI}), {FOS}: Computer and information sciences, {FOS}: Mathematics, Machine Learning (cs.{LG}), Machine Learning (stat.{ML}), Optimization and Control (math.{OC})},
}

@misc{gupta_shampoo_2018,
	title = {Shampoo: Preconditioned Stochastic Tensor Optimization},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1802.09568},
	doi = {10.48550/ARXIV.1802.09568},
	shorttitle = {Shampoo},
	abstract = {Preconditioned gradient methods are among the most general and powerful tools in optimization. However, preconditioning requires storing and manipulating prohibitively large matrices. We describe and analyze a new structure-aware preconditioning algorithm, called Shampoo, for stochastic optimization over tensor spaces. Shampoo maintains a set of preconditioning matrices, each of which operates on a single dimension, contracting over the remaining dimensions. We establish convergence guarantees in the stochastic convex setting, the proof of which builds upon matrix trace inequalities. Our experiments with state-of-the-art deep learning models show that Shampoo is capable of converging considerably faster than commonly used optimizers. Although it involves a more complex update rule, Shampoo's runtime per step is comparable to that of simple gradient methods such as {SGD}, {AdaGrad}, and Adam.},
	publisher = {{arXiv}},
	author = {Gupta, Vineet and Koren, Tomer and Singer, Yoram},
	urldate = {2026-02-24},
	date = {2018},
	note = {Version Number: 2},
	keywords = {{FOS}: Computer and information sciences, {FOS}: Mathematics, Machine Learning (cs.{LG}), Machine Learning (stat.{ML}), Optimization and Control (math.{OC})},
}

@misc{vyas_soap_2024,
	title = {{SOAP}: Improving and Stabilizing Shampoo using Adam},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2409.11321},
	doi = {10.48550/ARXIV.2409.11321},
	shorttitle = {{SOAP}},
	abstract = {There is growing evidence of the effectiveness of Shampoo, a higher-order preconditioning method, over Adam in deep learning optimization tasks. However, Shampoo's drawbacks include additional hyperparameters and computational overhead when compared to Adam, which only updates running averages of first- and second-moment quantities. This work establishes a formal connection between Shampoo (implemented with the 1/2 power) and Adafactor -- a memory-efficient approximation of Adam -- showing that Shampoo is equivalent to running Adafactor in the eigenbasis of Shampoo's preconditioner. This insight leads to the design of a simpler and computationally efficient algorithm: \${\textbackslash}textbf\{S\}\$hampo\${\textbackslash}textbf\{O\}\$ with \${\textbackslash}textbf\{A\}\$dam in the \${\textbackslash}textbf\{P\}\$reconditioner's eigenbasis ({SOAP}).
 With regards to improving Shampoo's computational efficiency, the most straightforward approach would be to simply compute Shampoo's eigendecomposition less frequently. Unfortunately, as our empirical results show, this leads to performance degradation that worsens with this frequency. {SOAP} mitigates this degradation by continually updating the running average of the second moment, just as Adam does, but in the current (slowly changing) coordinate basis. Furthermore, since {SOAP} is equivalent to running Adam in a rotated space, it introduces only one additional hyperparameter (the preconditioning frequency) compared to Adam. We empirically evaluate {SOAP} on language model pre-training with 360m and 660m sized models. In the large batch regime, {SOAP} reduces the number of iterations by over 40\% and wall clock time by over 35\% compared to {AdamW}, with approximately 20\% improvements in both metrics compared to Shampoo. An implementation of {SOAP} is available at https://github.com/nikhilvyas/{SOAP}.},
	publisher = {{arXiv}},
	author = {Vyas, Nikhil and Morwani, Depen and Zhao, Rosie and Kwun, Mujin and Shapira, Itai and Brandfonbrener, David and Janson, Lucas and Kakade, Sham},
	urldate = {2026-02-24},
	date = {2024},
	note = {Version Number: 2},
	keywords = {Artificial Intelligence (cs.{AI}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
}

@misc{shazeer_adafactor_2018,
	title = {Adafactor: Adaptive Learning Rates with Sublinear Memory Cost},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1804.04235},
	doi = {10.48550/ARXIV.1804.04235},
	shorttitle = {Adafactor},
	abstract = {In several recently proposed stochastic optimization methods (e.g. {RMSProp}, Adam, Adadelta), parameter updates are scaled by the inverse square roots of exponential moving averages of squared past gradients. Maintaining these per-parameter second-moment estimators requires memory equal to the number of parameters. For the case of neural network weight matrices, we propose maintaining only the per-row and per-column sums of these moving averages, and estimating the per-parameter second moments based on these sums. We demonstrate empirically that this method produces similar results to the baseline. Secondly, we show that adaptive methods can produce larger-than-desired updates when the decay rate of the second moment accumulator is too slow. We propose update clipping and a gradually increasing decay rate scheme as remedies. Combining these methods and dropping momentum, we achieve comparable results to the published Adam regime in training the Transformer model on the {WMT} 2014 English-German machine translation task, while using very little auxiliary storage in the optimizer. Finally, we propose scaling the parameter updates based on the scale of the parameters themselves.},
	publisher = {{arXiv}},
	author = {Shazeer, Noam and Stern, Mitchell},
	urldate = {2026-02-24},
	date = {2018},
	note = {Version Number: 1},
	keywords = {Artificial Intelligence (cs.{AI}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Machine Learning (stat.{ML})},
}

@misc{zhao_galore_2024,
	title = {{GaLore}: Memory-Efficient {LLM} Training by Gradient Low-Rank Projection},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2403.03507},
	doi = {10.48550/ARXIV.2403.03507},
	shorttitle = {{GaLore}},
	abstract = {Training Large Language Models ({LLMs}) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation ({LoRA}), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection ({GaLore}), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as {LoRA}. Our approach reduces memory usage by up to 65.5\% in optimizer states while maintaining both efficiency and performance for pre-training on {LLaMA} 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning {RoBERTa} on {GLUE} tasks. Our 8-bit {GaLore} further reduces optimizer memory by up to 82.5\% and total training memory by 63.3\%, compared to a {BF}16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer {GPUs} with 24GB memory (e.g., {NVIDIA} {RTX} 4090) without model parallel, checkpointing, or offloading strategies.},
	publisher = {{arXiv}},
	author = {Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
	urldate = {2026-02-24},
	date = {2024},
	note = {Version Number: 2},
	keywords = {{FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
}

@misc{mishchenko_prodigy_2023,
	title = {Prodigy: An Expeditiously Adaptive Parameter-Free Learner},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2306.06101},
	doi = {10.48550/ARXIV.2306.06101},
	shorttitle = {Prodigy},
	abstract = {We consider the problem of estimating the learning rate in adaptive methods, such as {AdaGrad} and Adam. We propose Prodigy, an algorithm that provably estimates the distance to the solution \$D\$, which is needed to set the learning rate optimally. At its core, Prodigy is a modification of the D-Adaptation method for learning-rate-free learning. It improves upon the convergence rate of D-Adaptation by a factor of \$O({\textbackslash}sqrt\{{\textbackslash}log(D/d\_0)\})\$, where \$d\_0\$ is the initial estimate of \$D\$. We test Prodigy on 12 common logistic-regression benchmark datasets, {VGG}11 and {ResNet}-50 training on {CIFAR}10, {ViT} training on Imagenet, {LSTM} training on {IWSLT}14, {DLRM} training on Criteo dataset, {VarNet} on Knee {MRI} dataset, as well as {RoBERTa} and {GPT} transformer training on {BookWiki}. Our experimental results show that our approach consistently outperforms D-Adaptation and reaches test accuracy values close to that of hand-tuned Adam.},
	publisher = {{arXiv}},
	author = {Mishchenko, Konstantin and Defazio, Aaron},
	urldate = {2026-02-24},
	date = {2023},
	note = {Version Number: 4},
	keywords = {Artificial Intelligence (cs.{AI}), {FOS}: Computer and information sciences, {FOS}: Mathematics, Machine Learning (cs.{LG}), Machine Learning (stat.{ML}), Optimization and Control (math.{OC})},
}

@misc{chen_symbolic_2023,
	title = {Symbolic Discovery of Optimization Algorithms},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2302.06675},
	doi = {10.48550/ARXIV.2302.06675},
	abstract = {We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, \${\textbackslash}textbf\{Lion\}\$ (\${\textbackslash}textit\{Evo\${\textbackslash}textbf\{L\}\$ved S\${\textbackslash}textbf\{i\}\$gn M\${\textbackslash}textbf\{o\}\$me\${\textbackslash}textbf\{n\}\$tum\}\$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of {ViT} by up to 2\% on {ImageNet} and saves up to 5x the pre-training compute on {JFT}. On vision-language contrastive learning, we achieve 88.3\% \${\textbackslash}textit\{zero-shot\}\$ and 91.1\% \${\textbackslash}textit\{fine-tuning\}\$ accuracy on {ImageNet}, surpassing the previous best results by 2\% and 0.1\%, respectively. On diffusion models, Lion outperforms Adam by achieving a better {FID} score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads {CTR} model.},
	publisher = {{arXiv}},
	author = {Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Liu, Yao and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu, Yifeng and Le, Quoc V.},
	urldate = {2026-02-24},
	date = {2023},
	note = {Version Number: 4},
	keywords = {Artificial Intelligence (cs.{AI}), Computation and Language (cs.{CL}), Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Neural and Evolutionary Computing (cs.{NE})},
}

@misc{liu_sophia_2023,
	title = {Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2305.14342},
	doi = {10.48550/ARXIV.2305.14342},
	shorttitle = {Sophia},
	abstract = {Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with {GPT} models of sizes ranging from 125M to 1.5B, Sophia achieves a 2x speed-up compared to Adam in the number of steps, total compute, and wall-clock time, achieving the same perplexity with 50\% fewer steps, less total compute, and reduced wall-clock time. Theoretically, we show that Sophia, in a much simplified setting, adapts to the heterogeneous curvatures in different parameter dimensions, and thus has a run-time bound that does not depend on the condition number of the loss.},
	publisher = {{arXiv}},
	author = {Liu, Hong and Li, Zhiyuan and Hall, David and Liang, Percy and Ma, Tengyu},
	urldate = {2026-02-24},
	date = {2023},
	note = {Version Number: 4},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, {FOS}: Mathematics, Machine Learning (cs.{LG}), Optimization and Control (math.{OC})},
}

@misc{shen_convergence_2025,
	title = {On the Convergence Analysis of Muon},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2505.23737},
	doi = {10.48550/ARXIV.2505.23737},
	abstract = {The majority of parameters in neural networks are naturally represented as matrices. However, most commonly used optimizers treat these matrix parameters as flattened vectors during optimization, potentially overlooking their inherent structural properties. Recently, an optimizer called Muon has been proposed, specifically designed to optimize matrix-structured parameters. Extensive empirical evidence shows that Muon can significantly outperform traditional optimizers when training neural networks. Nonetheless, the theoretical understanding of Muon's convergence behavior and the reasons behind its superior performance remain limited. In this work, we present a comprehensive convergence rate analysis of Muon and its comparison with Gradient Descent ({GD}). We further characterize the conditions under which Muon can outperform {GD}. Our theoretical results reveal that Muon can benefit from the low-rank and approximate blockwise diagonal structure of Hessian matrices -- phenomena widely observed in practical neural network training. Our experimental results support and corroborate the theoretical findings.},
	publisher = {{arXiv}},
	author = {Shen, Wei and Huang, Ruichuan and Huang, Minhui and Shen, Cong and Zhang, Jiawei},
	urldate = {2026-02-24},
	date = {2025},
	note = {Version Number: 1},
	keywords = {{FOS}: Computer and information sciences, {FOS}: Mathematics, Information Theory (cs.{IT}), Machine Learning (cs.{LG}), Machine Learning (stat.{ML}), Optimization and Control (math.{OC})},
}

@misc{boissin_turbo-muon_2025,
	title = {Turbo-Muon: Accelerating Orthogonality-Based Optimization with Pre-Conditioning},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2512.04632},
	doi = {10.48550/ARXIV.2512.04632},
	shorttitle = {Turbo-Muon},
	abstract = {Orthogonality-based optimizers, such as Muon, have recently shown strong performance across large-scale training and community-driven efficiency challenges. However, these methods rely on a costly gradient orthogonalization step. Even efficient iterative approximations such as Newton-Schulz remain expensive, typically requiring dozens of matrix multiplications to converge. We introduce a preconditioning procedure that accelerates Newton-Schulz convergence and reduces its computational cost. We evaluate its impact and show that the overhead of our preconditioning can be made negligible. Furthermore, the faster convergence it enables allows us to remove one iteration out of the usual five without degrading approximation quality. Our publicly available implementation achieves up to a 2.8x speedup in the Newton-Schulz approximation. We also show that this has a direct impact on end-to-end training runtime with 5-10\% improvement in realistic training scenarios across two efficiency-focused tasks. On challenging language or vision tasks, we validate that our method maintains equal or superior model performance while improving runtime. Crucially, these improvements require no hyperparameter tuning and can be adopted as a simple drop-in replacement. Our code is publicly available on github.},
	publisher = {{arXiv}},
	author = {Boissin, Thibaut and Massena, Thomas and Mamalet, Franck and Serrurier, Mathieu},
	urldate = {2026-02-24},
	date = {2025},
	note = {Version Number: 1},
	keywords = {Artificial Intelligence (cs.{AI}), {FOS}: Computer and information sciences},
}
