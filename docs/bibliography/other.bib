
@article{zhang_continuous_2021,
	title = {Continuous Self-Attention Models with Neural {ODE} Networks},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17692},
	doi = {10.1609/aaai.v35i16.17692},
	abstract = {Stacked self-attention models receive widespread attention, due to its ability of capturing global dependency among words. However, the stacking of many layers and components generates huge parameters, leading to low parameter efficiency. In response to this issue, we propose a lightweight architecture named Continuous Self-Attention models with neural {ODE} networks ({CSAODE}). In {CSAODE}, continuous dynamical models (i.e., neural {ODEs}) are coupled with our proposed self-attention block to form a self-attention {ODE} solver. This solver  continuously calculates and optimizes the hidden states via only one layer of parameters to improve the  parameter efficiency. In addition, we design a novel accelerated continuous dynamical model to reduce computing costs, and integrate it in  {CSAODE}. Moreover, since the original self-attention ignores local information,  {CSAODE} makes use of N-gram convolution to encode local representations, and a fusion layer with only two trainable scalars are designed for generating sentence vectors. We perform a series of experiments on text classification, neural language inference ({NLI}) and text matching tasks. With fewer parameters, {CSAODE} outperforms state-of-the-art models on text classification tasks (e.g.,  1.3\% accuracy improved on {SUBJ} task), and has competitive performances for {NLI} and text matching tasks as well.},
	pages = {14393--14401},
	number = {16},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Zhang, Jing and Zhang, Peng and Kong, Baiwen and Wei, Junqiu and Jiang, Xin},
	urldate = {2026-02-15},
	date = {2021-05-18},
	langid = {english},
	file = {PDF:C\:\\Users\\erick.merino\\Zotero\\storage\\NFVQPMNN\\Zhang et al. - 2021 - Continuous Self-Attention Models with Neural ODE Networks.pdf:application/pdf},
}

@misc{sun_mobilebert_2020,
	title = {{MobileBERT}: a Compact Task-Agnostic {BERT} for Resource-Limited Devices},
	url = {http://arxiv.org/abs/2004.02984},
	doi = {10.48550/arXiv.2004.02984},
	shorttitle = {{MobileBERT}},
	abstract = {Natural Language Processing ({NLP}) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. In this paper, we propose {MobileBERT} for compressing and accelerating the popular {BERT} model. Like the original {BERT}, {MobileBERT} is task-agnostic, that is, it can be generically applied to various downstream {NLP} tasks via simple fine-tuning. Basically, {MobileBERT} is a thin version of {BERT}\_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train {MobileBERT}, we first train a specially designed teacher model, an inverted-bottleneck incorporated {BERT}\_LARGE model. Then, we conduct knowledge transfer from this teacher to {MobileBERT}. Empirical studies show that {MobileBERT} is 4.3x smaller and 5.5x faster than {BERT}\_BASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of {GLUE}, {MobileBERT} achieves a {GLUEscore} o 77.7 (0.6 lower than {BERT}\_BASE), and 62 ms latency on a Pixel 4 phone. On the {SQuAD} v1.1/v2.0 question answering task, {MobileBERT} achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than {BERT}\_BASE).},
	number = {{arXiv}:2004.02984},
	author = {Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny},
	urldate = {2026-02-15},
	date = {2020-04-14},
	eprinttype = {arxiv},
	eprint = {2004.02984 [cs]},
	file = {Sun et al. - 2020 - MobileBERT a Compact Task-Agnostic BERT for Resource-Limited Devices.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\CAKKNPHF\\Sun et al. - 2020 - MobileBERT a Compact Task-Agnostic BERT for Resource-Limited Devices.pdf:application/pdf},
}

@misc{yang_looped_2024,
	title = {Looped Transformers are Better at Learning Learning Algorithms},
	url = {http://arxiv.org/abs/2311.12424},
	doi = {10.48550/arXiv.2311.12424},
	abstract = {Transformers have demonstrated effectiveness in in-context solving data-fitting problems from various (latent) models, as reported by Garg et al. However, the absence of an inherent iterative structure in the transformer architecture presents a challenge in emulating the iterative algorithms, which are commonly employed in traditional machine learning methods. To address this, we propose the utilization of looped transformer architecture and its associated training methodology, with the aim of incorporating iterative characteristics into the transformer architectures. Experimental results suggest that the looped transformer achieves performance comparable to the standard transformer in solving various data-fitting problems, while utilizing less than 10\% of the parameter count.},
	number = {{arXiv}:2311.12424},
	author = {Yang, Liu and Lee, Kangwook and Nowak, Robert and Papailiopoulos, Dimitris},
	urldate = {2026-02-14},
	date = {2024-03-16},
	eprinttype = {arxiv},
	eprint = {2311.12424 [cs]},
	file = {Yang et al. - 2024 - Looped Transformers are Better at Learning Learning Algorithms.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\6HZX3NDS\\Yang et al. - 2024 - Looped Transformers are Better at Learning Learning Algorithms.pdf:application/pdf},
}

@misc{reimers_sentence-bert_2019,
	title = {Sentence-{BERT}: Sentence Embeddings using Siamese {BERT}-Networks},
	url = {http://arxiv.org/abs/1908.10084},
	doi = {10.48550/arXiv.1908.10084},
	shorttitle = {Sentence-{BERT}},
	abstract = {{BERT} (Devlin et al., 2018) and {RoBERTa} (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity ({STS}). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with {BERT}. The construction of {BERT} makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-{BERT} ({SBERT}), a modification of the pretrained {BERT} network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with {BERT} / {RoBERTa} to about 5 seconds with {SBERT}, while maintaining the accuracy from {BERT}. We evaluate {SBERT} and {SRoBERTa} on common {STS} tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
	number = {{arXiv}:1908.10084},
	author = {Reimers, Nils and Gurevych, Iryna},
	urldate = {2026-02-14},
	date = {2019-08-27},
	eprinttype = {arxiv},
	eprint = {1908.10084 [cs]},
	file = {Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese BERT-Networks.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\KDW449L8\\Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese BERT-Networks.pdf:application/pdf},
}

@misc{wang_bitnet_2023,
	title = {{BitNet}: Scaling 1-bit Transformers for Large Language Models},
	url = {http://arxiv.org/abs/2310.11453},
	doi = {10.48550/arXiv.2310.11453},
	shorttitle = {{BitNet}},
	abstract = {The increasing size of large language models has posed challenges for deployment and raised concerns about environmental impact due to high energy consumption. In this work, we introduce {BitNet}, a scalable and stable 1-bit Transformer architecture designed for large language models. Specifically, we introduce {BitLinear} as a drop-in replacement of the nn.Linear layer in order to train 1-bit weights from scratch. Experimental results on language modeling show that {BitNet} achieves competitive performance while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods and {FP}16 Transformer baselines. Furthermore, {BitNet} exhibits a scaling law akin to full-precision Transformers, suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.},
	number = {{arXiv}:2310.11453},
	author = {Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Wang, Huaijie and Ma, Lingxiao and Yang, Fan and Wang, Ruiping and Wu, Yi and Wei, Furu},
	urldate = {2026-02-14},
	date = {2023-10-17},
	eprinttype = {arxiv},
	eprint = {2310.11453 [cs]},
	file = {Wang et al. - 2023 - BitNet Scaling 1-bit Transformers for Large Language Models.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\G9S4V2L4\\Wang et al. - 2023 - BitNet Scaling 1-bit Transformers for Large Language Models.pdf:application/pdf},
}

@misc{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	number = {{arXiv}:1810.04805},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2026-02-14},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1810.04805 [cs]},
	file = {Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\ILD3FIKI\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:application/pdf},
}

@misc{canete_spanish_2023,
	title = {Spanish Pre-trained {BERT} Model and Evaluation Data},
	url = {http://arxiv.org/abs/2308.02976},
	doi = {10.48550/arXiv.2308.02976},
	abstract = {The Spanish language is one of the top 5 spoken languages in the world. Nevertheless, finding resources to train or evaluate Spanish language models is not an easy task. In this paper we help bridge this gap by presenting a {BERT}-based language model pre-trained exclusively on Spanish data. As a second contribution, we also compiled several tasks specifically for the Spanish language in a single repository much in the spirit of the {GLUE} benchmark. By fine-tuning our pre-trained Spanish model, we obtain better results compared to other {BERT}-based models pre-trained on multilingual corpora for most of the tasks, even achieving a new state-of-the-art on some of them. We have publicly released our model, the pre-training data, and the compilation of the Spanish benchmarks.},
	number = {{arXiv}:2308.02976},
	author = {Cañete, José and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and Pérez, Jorge},
	urldate = {2026-02-14},
	date = {2023-08-06},
	eprinttype = {arxiv},
	eprint = {2308.02976 [cs]},
	file = {Cañete et al. - 2023 - Spanish Pre-trained BERT Model and Evaluation Data.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\6ZBCA4TI\\Cañete et al. - 2023 - Spanish Pre-trained BERT Model and Evaluation Data.pdf:application/pdf},
}

@article{cai_survey_2025,
	title = {A Survey on Mixture of Experts in Large Language Models},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {http://arxiv.org/abs/2407.06204},
	doi = {10.1109/TKDE.2025.3554028},
	abstract = {Large language models ({LLMs}) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of {LLMs} is underpinned by their substantial model size, extensive and diverse datasets, and the vast computational power harnessed during training, all of which contribute to the emergent abilities of {LLMs} (e.g., in-context learning) that are not present in small models. Within this context, the mixture of experts ({MoE}) has emerged as an effective method for substantially scaling up model capacity with minimal computation overhead, gaining significant attention from academia and industry. Despite its growing prevalence, there lacks a systematic and comprehensive review of the literature on {MoE}. This survey seeks to bridge that gap, serving as an essential resource for researchers delving into the intricacies of {MoE}. We first briefly introduce the structure of the {MoE} layer, followed by proposing a new taxonomy of {MoE}. Next, we overview the core designs for various {MoE} models including both algorithmic and systemic aspects, alongside collections of available open-source implementations, hyperparameter configurations and empirical evaluations. Furthermore, we delineate the multifaceted applications of {MoE} in practice, and outline some potential directions for future research. To facilitate ongoing updates and the sharing of cutting-edge advances in {MoE} research, we have established a resource repository at https://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts-in-{LLMs}.},
	pages = {1--20},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Cai, Weilin and Jiang, Juyong and Wang, Fan and Tang, Jing and Kim, Sunghun and Huang, Jiayi},
	urldate = {2026-02-14},
	date = {2025},
	eprinttype = {arxiv},
	eprint = {2407.06204 [cs]},
	file = {Cai et al. - 2025 - A Survey on Mixture of Experts in Large Language Models.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\QX7WCHBA\\Cai et al. - 2025 - A Survey on Mixture of Experts in Large Language Models.pdf:application/pdf},
}

@misc{yang_survey_2025,
	title = {A Survey of Retentive Network},
	url = {http://arxiv.org/abs/2506.06708},
	doi = {10.48550/arXiv.2506.06708},
	abstract = {Retentive Network ({RetNet}) represents a significant advancement in neural network architecture, offering an efficient alternative to the Transformer. While Transformers rely on self-attention to model dependencies, they suffer from high memory costs and limited scalability when handling long sequences due to their quadratic complexity. To mitigate these limitations, {RetNet} introduces a retention mechanism that unifies the inductive bias of recurrence with the global dependency modeling of attention. This mechanism enables linear-time inference, facilitates efficient modeling of extended contexts, and remains compatible with fully parallelizable training pipelines. {RetNet} has garnered significant research interest due to its consistently demonstrated cross-domain effectiveness, achieving robust performance across machine learning paradigms including natural language processing, speech recognition, and time-series analysis. However, a comprehensive review of {RetNet} is still missing from the current literature. This paper aims to fill that gap by offering the first detailed survey of the {RetNet} architecture, its key innovations, and its diverse applications. We also explore the main challenges associated with {RetNet} and propose future research directions to support its continued advancement in both academic research and practical deployment.},
	number = {{arXiv}:2506.06708},
	author = {Yang, Haiqi and Li, Zhiyuan and Chang, Yi and Wu, Yuan},
	urldate = {2026-02-14},
	date = {2025-06-07},
	eprinttype = {arxiv},
	eprint = {2506.06708 [cs]},
	file = {Yang et al. - 2025 - A Survey of Retentive Network.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\8LVHHGFV\\Yang et al. - 2025 - A Survey of Retentive Network.pdf:application/pdf},
}

@misc{behrouz_titans_2024,
	title = {Titans: Learning to Memorize at Test Time},
	url = {http://arxiv.org/abs/2501.00663},
	doi = {10.48550/arXiv.2501.00663},
	shorttitle = {Titans},
	abstract = {Over more than a decade there has been an extensive research effort on how to effectively utilize recurrent models and attention. While recurrent models aim to compress the data into a fixed-size memory (called hidden state), attention allows attending to the entire context window, capturing the direct dependencies of all tokens. This more accurate modeling of dependencies, however, comes with a quadratic cost, limiting the model to a fixed-length context. We present a new neural long-term memory module that learns to memorize historical context and helps attention to attend to the current context while utilizing long past information. We show that this neural memory has the advantage of fast parallelizable training while maintaining a fast inference. From a memory perspective, we argue that attention due to its limited context but accurate dependency modeling performs as a short-term memory, while neural memory due to its ability to memorize the data, acts as a long-term, more persistent, memory. Based on these two modules, we introduce a new family of architectures, called Titans, and present three variants to address how one can effectively incorporate memory into this architecture. Our experimental results on language modeling, common-sense reasoning, genomics, and time series tasks show that Titans are more effective than Transformers and recent modern linear recurrent models. They further can effectively scale to larger than 2M context window size with higher accuracy in needle-in-haystack tasks compared to baselines.},
	number = {{arXiv}:2501.00663},
	author = {Behrouz, Ali and Zhong, Peilin and Mirrokni, Vahab},
	urldate = {2026-02-14},
	date = {2024-12-31},
	eprinttype = {arxiv},
	eprint = {2501.00663 [cs]},
	file = {Behrouz et al. - 2024 - Titans Learning to Memorize at Test Time.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\IHYAI24P\\Behrouz et al. - 2024 - Titans Learning to Memorize at Test Time.pdf:application/pdf},
}

@misc{gillin_bert-jepa_2026,
	title = {{BERT}-{JEPA}: Reorganizing {CLS} Embeddings for Language-Invariant Semantics},
	url = {http://arxiv.org/abs/2601.00366},
	doi = {10.48550/arXiv.2601.00366},
	shorttitle = {{BERT}-{JEPA}},
	abstract = {Joint Embedding Predictive Architectures ({JEPA}) are a novel self supervised training technique that have shown recent promise across domains. We introduce {BERT}-{JEPA} ({BEPA}), a training paradigm that adds a {JEPA} training objective to {BERT}-style models, working to combat a collapsed [{CLS}] embedding space and turning it into a language-agnostic space. This new structure leads to increased performance across multilingual benchmarks.},
	number = {{arXiv}:2601.00366},
	author = {Gillin, Taj and Lalani, Adam and Zhang, Kenneth and Salles, Marcel Mateos},
	urldate = {2026-02-14},
	date = {2026-01-01},
	eprinttype = {arxiv},
	eprint = {2601.00366 [cs]},
	file = {Gillin et al. - 2026 - BERT-JEPA Reorganizing CLS Embeddings for Language-Invariant Semantics.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\J489YDE6\\Gillin et al. - 2026 - BERT-JEPA Reorganizing CLS Embeddings for Language-Invariant Semantics.pdf:application/pdf},
}

@misc{bui_knowledge_2024,
	title = {Knowledge Distillation vs. Pretraining from Scratch under a Fixed (Computation) Budget},
	url = {http://arxiv.org/abs/2404.19319},
	doi = {10.48550/arXiv.2404.19319},
	abstract = {Compared to standard language model ({LM}) pretraining (i.e., from scratch), Knowledge Distillation ({KD}) entails an additional forward pass through a teacher model that is typically substantially larger than the target student model. As such, {KD} in {LM} pretraining materially slows down throughput of pretraining instances vis-a-vis pretraining from scratch. Scaling laws of {LM} pretraining suggest that smaller models can close the gap to larger counterparts if trained on more data (i.e., processing more tokens)-and under a fixed computation budget, smaller models are able be process more data than larger models. We thus hypothesize that {KD} might, in fact, be suboptimal to pretraining from scratch for obtaining smaller {LMs}, when appropriately accounting for the compute budget. To test this, we compare pretraining from scratch against several {KD} strategies for masked language modeling ({MLM}) in a fair experimental setup, with respect to amount of computation as well as pretraining data. Downstream results on {GLUE}, however, do not confirm our hypothesis: while pretraining from scratch performs comparably to ordinary {KD} under a fixed computation budget, more sophisticated {KD} strategies, namely {TinyBERT} (Jiao et al., 2020) and {MiniLM} (Wang et al., 2023), outperform it by a notable margin. We further find that {KD} yields larger gains over pretraining from scratch when the data must be repeated under the fixed computation budget.},
	number = {{arXiv}:2404.19319},
	author = {Bui, Minh Duc and Schmidt, Fabian David and Glavaš, Goran and Wense, Katharina von der},
	urldate = {2026-02-14},
	date = {2024-04-30},
	eprinttype = {arxiv},
	eprint = {2404.19319 [cs]},
	file = {Bui et al. - 2024 - Knowledge Distillation vs. Pretraining from Scratch under a Fixed (Computation) Budget.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\8S36PG29\\Bui et al. - 2024 - Knowledge Distillation vs. Pretraining from Scratch under a Fixed (Computation) Budget.pdf:application/pdf},
}

@misc{samson_lightweight_2026,
	title = {Lightweight Transformer Architectures for Edge Devices in Real-Time Applications},
	url = {http://arxiv.org/abs/2601.03290},
	doi = {10.48550/arXiv.2601.03290},
	abstract = {The deployment of transformer-based models on resource-constrained edge devices represents a critical challenge in enabling real-time artificial intelligence applications. This comprehensive survey examines lightweight transformer architectures specifically designed for edge deployment, analyzing recent advances in model compression, quantization, pruning, and knowledge distillation techniques. We systematically review prominent lightweight variants including {MobileBERT}, {TinyBERT}, {DistilBERT}, {EfficientFormer}, {EdgeFormer}, and {MobileViT}, providing detailed performance benchmarks on standard datasets such as {GLUE}, {SQuAD}, {ImageNet}-1K, and {COCO}. Our analysis encompasses current industry adoption patterns across major hardware platforms ({NVIDIA} Jetson, Qualcomm Snapdragon, Apple Neural Engine, {ARM} architectures), deployment frameworks ({TensorFlow} Lite, {ONNX} Runtime, {PyTorch} Mobile, {CoreML}), and optimization strategies. Experimental results demonstrate that modern lightweight transformers can achieve 75-96\% of full-model accuracy while reducing model size by 4-10x and inference latency by 3-9x, enabling deployment on devices with as little as 2-5W power consumption. We identify sparse attention mechanisms, mixed-precision quantization ({INT}8/{FP}16), and hardware-aware neural architecture search as the most effective optimization strategies. Novel findings include memory-bandwidth bottleneck analysis revealing 15-40M parameter models achieve optimal hardware utilization (60-75\% efficiency), quantization sweet spots for different model types, and comprehensive energy efficiency profiling across edge platforms. We establish real-time performance boundaries and provide a practical 6-step deployment pipeline achieving 8-12x size reduction with less than 2\% accuracy degradation.},
	number = {{arXiv}:2601.03290},
	author = {Samson, Hema Hariharan},
	urldate = {2026-02-14},
	date = {2026-01-05},
	eprinttype = {arxiv},
	eprint = {2601.03290 [cs]},
	file = {Samson - 2026 - Lightweight Transformer Architectures for Edge Devices in Real-Time Applications.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\XJKPTUFZ\\Samson - 2026 - Lightweight Transformer Architectures for Edge Devices in Real-Time Applications.pdf:application/pdf},
}

@misc{zhu_transformers_2025,
	title = {Transformers without Normalization},
	url = {http://arxiv.org/abs/2503.10622},
	doi = {10.48550/arXiv.2503.10622},
	abstract = {Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh ({DyT}), an element-wise operation \${DyT}(\$x\$) = {\textbackslash}tanh(α\$x\$)\$, as a drop-in replacement for normalization layers in Transformers. {DyT} is inspired by the observation that layer normalization in Transformers often produces tanh-like, \$S\$-shaped input-output mappings. By incorporating {DyT}, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with {DyT} across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.},
	number = {{arXiv}:2503.10622},
	author = {Zhu, Jiachen and Chen, Xinlei and He, Kaiming and {LeCun}, Yann and Liu, Zhuang},
	urldate = {2026-02-14},
	date = {2025-06-14},
	eprinttype = {arxiv},
	eprint = {2503.10622 [cs]},
	file = {Zhu et al. - 2025 - Transformers without Normalization.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\NEWKRJRN\\Zhu et al. - 2025 - Transformers without Normalization.pdf:application/pdf},
}

@misc{chen_stronger_2025,
	title = {Stronger Normalization-Free Transformers},
	url = {http://arxiv.org/abs/2512.10938},
	doi = {10.48550/arXiv.2512.10938},
	abstract = {Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh ({DyT}) has demonstrated that alternatives are possible. The point-wise function {DyT} constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce \${\textbackslash}mathrm\{Derf\}(x) = {\textbackslash}mathrm\{erf\}(αx + s)\$, where \${\textbackslash}mathrm\{erf\}(x)\$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms {LayerNorm}, {RMSNorm}, and {DyT} across a wide range of domains, including vision (image recognition and generation), speech representation, and {DNA} sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.},
	number = {{arXiv}:2512.10938},
	author = {Chen, Mingzhi and Lu, Taiming and Zhu, Jiachen and Sun, Mingjie and Liu, Zhuang},
	urldate = {2026-02-14},
	date = {2025-12-11},
	eprinttype = {arxiv},
	eprint = {2512.10938 [cs]},
	file = {Chen et al. - 2025 - Stronger Normalization-Free Transformers.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\7EHYQLJW\\Chen et al. - 2025 - Stronger Normalization-Free Transformers.pdf:application/pdf},
}

@misc{yongueng_holonorm_2025,
	title = {Holonorm},
	url = {http://arxiv.org/abs/2511.10504},
	doi = {10.48550/arXiv.2511.10504},
	abstract = {Normalization is a key point in transformer training . In Dynamic Tanh ({DyT}), the author demonstrated that Tanh can be used as an alternative layer normalization ({LN}) and confirmed the effectiveness of the idea. But Tanh itself faces orthogonality, linearity and distortion problems. Due to that, his proposition cannot be reliable. So we propose a Holonorm (hn) which has residual connections and nonlinearity. Holonorm is suitable for replacing Tanh in the context of normalization. Although the {HoloNorm} expression could be similar to the softsign function in dimension one, softsign is a componentwise function which is not good for tensors and vectors of great dimension. Holonorm preserves the orthogonality, the direction, the invertibility of the signal. Holonorm is also a suitable metric, maps all vectors into the open unit ball. This prevents exploding activations and improves stability in deep Transformer models. In this work, we have meticulously examined the normalization in transformers and say that Holonorm, a generalized form of softsign function suited as a normalization function first.Second, defined between 0 and 1 hn serves as a percentage, and \$1 - {\textbackslash}text\{Holonorm\}\$ is its complement, making it better understandable in evaluating a model.},
	number = {{arXiv}:2511.10504},
	author = {Yongueng, Daryl Noupa and Tembine, Hamidou},
	urldate = {2026-02-14},
	date = {2025-11-13},
	eprinttype = {arxiv},
	eprint = {2511.10504 [cs]},
	file = {Yongueng and Tembine - 2025 - Holonorm.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\7XRR4FAK\\Yongueng and Tembine - 2025 - Holonorm.pdf:application/pdf},
}

@misc{bravin_embbert_2026,
	title = {{EmbBERT}: Attention Under 2 {MB} Memory},
	url = {http://arxiv.org/abs/2502.10001},
	doi = {10.48550/arXiv.2502.10001},
	shorttitle = {{EmbBERT}},
	abstract = {Transformer architectures based on the attention mechanism have revolutionized natural language processing ({NLP}), driving major breakthroughs across virtually every {NLP} task. However, their substantial memory and computational requirements still hinder deployment on ultra-constrained devices such as wearables and Internet-of-Things ({IoT}) units, where available memory is limited to just a few megabytes. To address this challenge, we introduce {EmbBERT}, a tiny language model ({TLM}) architecturally designed for extreme efficiency. The model integrates a compact embedding layer, streamlined feed-forward blocks, and an efficient attention mechanism that together enable optimal performance under strict memory budgets. Through this redesign for the extreme edge, we demonstrate that highly simplified transformer architectures remain remarkably effective under tight resource constraints. {EmbBERT} requires only 2 {MB} of total memory, and achieves accuracy performance comparable to the ones of state-of-the-art ({SotA}) models that require a \${\textbackslash}mathbf\{10{\textbackslash}times\}\$ memory budget. Extensive experiments on the curated {TinyNLP} benchmark and the {GLUE} suite confirm that {EmbBERT} achieves competitive accuracy, comparable to that of larger {SotA} models, and consistently outperforms downsized versions of {BERT} and {MAMBA} of similar size. Furthermore, we demonstrate the model resilience to 8-bit quantization, which further reduces memory usage to just 781 {kB} , and the scalability of the {EmbBERT} architecture across the sub-megabyte to tens-of-megabytes range. Finally, we perform an ablation study demonstrating the positive contributions of all components and the pre-training procedure. All code, scripts, and checkpoints are publicly released to ensure reproducibility: https://github.com/{RiccardoBravin}/tiny-{LLM}.},
	number = {{arXiv}:2502.10001},
	author = {Bravin, Riccardo and Pavan, Massimo and Shalby, Hazem Hesham Yousef and Pittorino, Fabrizio and Roveri, Manuel},
	urldate = {2026-02-14},
	date = {2026-02-11},
	eprinttype = {arxiv},
	eprint = {2502.10001 [cs]},
	file = {Bravin et al. - 2026 - EmbBERT Attention Under 2 MB Memory.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\RQ6D5FZ7\\Bravin et al. - 2026 - EmbBERT Attention Under 2 MB Memory.pdf:application/pdf},
}

@misc{hwang_hydra_2024,
	title = {Hydra: Bidirectional State Space Models Through Generalized Matrix Mixers},
	url = {http://arxiv.org/abs/2407.09941},
	doi = {10.48550/arXiv.2407.09941},
	shorttitle = {Hydra},
	abstract = {A wide array of sequence models are built on a framework modeled after Transformers, comprising alternating sequence mixer and channel mixer layers. This paper studies a unifying matrix mixer view of sequence mixers that can be conceptualized as a linear map on the input sequence. This framework encompasses a broad range of well-known sequence models, including the self-attention of Transformers as well as recent strong alternatives such as structured state space models ({SSMs}), and allows understanding downstream characteristics such as efficiency and expressivity through properties of their structured matrix class. We identify a key axis of matrix parameterizations termed sequence alignment, which increases the flexibility and performance of matrix mixers, providing insights into the strong performance of Transformers and recent {SSMs} such as Mamba. Furthermore, the matrix mixer framework offers a systematic approach to developing sequence mixers with desired properties, allowing us to develop several new sub-quadratic sequence models. In particular, we propose a natural bidirectional extension of the Mamba model (Hydra), parameterized as a quasiseparable matrix mixer, which demonstrates superior performance over other sequence models including Transformers on non-causal tasks. As a drop-in replacement for attention layers, Hydra outperforms {BERT} by 0.8 points on the {GLUE} benchmark and {ViT} by 2\% Top-1 accuracy on {ImageNet}.},
	number = {{arXiv}:2407.09941},
	author = {Hwang, Sukjun and Lahoti, Aakash and Dao, Tri and Gu, Albert},
	urldate = {2026-02-14},
	date = {2024-07-13},
	eprinttype = {arxiv},
	eprint = {2407.09941 [cs]},
	file = {Hwang et al. - 2024 - Hydra Bidirectional State Space Models Through Generalized Matrix Mixers.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\2CCBQ2I5\\Hwang et al. - 2024 - Hydra Bidirectional State Space Models Through Generalized Matrix Mixers.pdf:application/pdf},
}

@misc{dai_hope_2025,
	title = {{HoPE}: Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models},
	url = {http://arxiv.org/abs/2509.05218},
	doi = {10.48550/arXiv.2509.05218},
	shorttitle = {{HoPE}},
	abstract = {Positional encoding mechanisms enable Transformers to model sequential structure and long-range dependencies in text. While absolute positional encodings struggle with extrapolation to longer sequences due to fixed positional representations, and relative approaches like Alibi exhibit performance degradation on extremely long contexts, the widely-used Rotary Positional Encoding ({RoPE}) introduces oscillatory attention patterns that hinder stable long-distance dependency modelling. We address these limitations through a geometric reformulation of positional encoding. Drawing inspiration from Lorentz transformations in hyperbolic geometry, we propose Hyperbolic Rotary Positional Encoding ({HoPE}), which leverages hyperbolic functions to implement Lorentz rotations on token representations. Theoretical analysis demonstrates that {RoPE} is a special case of our generalized formulation. {HoPE} fundamentally resolves {RoPE}'s slation issues by enforcing monotonic decay of attention weights with increasing token distances. Extensive experimental results, including perplexity evaluations under several extended sequence benchmarks, show that {HoPE} consistently exceeds existing positional encoding methods. These findings underscore {HoPE}'s enhanced capacity for representing and generalizing long-range dependencies. Data and code will be available.},
	number = {{arXiv}:2509.05218},
	author = {Dai, Chang and Shan, Hongyu and Song, Mingyang and Liang, Di},
	urldate = {2026-02-14},
	date = {2025-09-08},
	eprinttype = {arxiv},
	eprint = {2509.05218 [cs]},
	file = {Dai et al. - 2025 - HoPE Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\SBA97FMQ\\Dai et al. - 2025 - HoPE Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models.pdf:application/pdf},
}

@misc{sun_retentive_2023,
	title = {Retentive Network: A Successor to Transformer for Large Language Models},
	url = {http://arxiv.org/abs/2307.08621},
	doi = {10.48550/arXiv.2307.08621},
	shorttitle = {Retentive Network},
	abstract = {In this work, we propose Retentive Network ({RetNet}) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost \$O(1)\$ inference, which improves decoding throughput, latency, and {GPU} memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that {RetNet} achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make {RetNet} a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.},
	number = {{arXiv}:2307.08621},
	author = {Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
	urldate = {2026-02-14},
	date = {2023-08-09},
	eprinttype = {arxiv},
	eprint = {2307.08621 [cs]},
	file = {Sun et al. - 2023 - Retentive Network A Successor to Transformer for Large Language Models.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\RC3UZT4G\\Sun et al. - 2023 - Retentive Network A Successor to Transformer for Large Language Models.pdf:application/pdf},
}

@misc{gu_mamba_2024,
	title = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
	url = {http://arxiv.org/abs/2312.00752},
	doi = {10.48550/arXiv.2312.00752},
	shorttitle = {Mamba},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models ({SSMs}) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the {SSM} parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective {SSMs} into a simplified end-to-end neural network architecture without attention or even {MLP} blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	number = {{arXiv}:2312.00752},
	author = {Gu, Albert and Dao, Tri},
	urldate = {2026-02-14},
	date = {2024-05-31},
	eprinttype = {arxiv},
	eprint = {2312.00752 [cs]},
	file = {Gu and Dao - 2024 - Mamba Linear-Time Sequence Modeling with Selective State Spaces.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\LEAABUJA\\Gu and Dao - 2024 - Mamba Linear-Time Sequence Modeling with Selective State Spaces.pdf:application/pdf},
}

@misc{zhang_why_2020,
	title = {Why gradient clipping accelerates training: A theoretical justification for adaptivity},
	url = {http://arxiv.org/abs/1905.11881},
	doi = {10.48550/arXiv.1905.11881},
	shorttitle = {Why gradient clipping accelerates training},
	abstract = {We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, {\textbackslash}emph\{gradient clipping\} and {\textbackslash}emph\{normalized gradient\}, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.},
	number = {{arXiv}:1905.11881},
	author = {Zhang, Jingzhao and He, Tianxing and Sra, Suvrit and Jadbabaie, Ali},
	urldate = {2026-02-14},
	date = {2020-02-10},
	eprinttype = {arxiv},
	eprint = {1905.11881 [math]},
	file = {Zhang et al. - 2020 - Why gradient clipping accelerates training A theoretical justification for adaptivity.pdf:C\:\\Users\\erick.merino\\Zotero\\storage\\GH8JDCC6\\Zhang et al. - 2020 - Why gradient clipping accelerates training A theoretical justification for adaptivity.pdf:application/pdf},
}

@misc{zhang_root_2019,
	title = {{Root Mean Square Layer Normalization}},
	url = {http://arxiv.org/abs/1910.07467},
	doi = {10.48550/arXiv.1910.07467},
	shorttitle = {{RMSNorm}},
	author = {Zhang, Biao and Sennrich, Rico},
	urldate = {2026-02-24},
	date = {2019-10-16},
	eprinttype = {arxiv},
	eprint = {1910.07467 [cs]},
}
