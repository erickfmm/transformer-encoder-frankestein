model_class: frankenstein
model:
  vocab_size: 50000
  hidden_size: 768
  num_layers: 6
  num_loops: 1
  num_heads: 12
  retention_heads: 12
  num_experts: 4
  top_k_experts: 2
  dropout: 0.1
  ode_solver: rk4
  ode_steps: 2
  use_bitnet: false
  norm_type: layer_norm
  layer_pattern:
    - standard_attn
  use_factorized_embedding: false
  factorized_embedding_dim: 128
  use_embedding_conv: false
  embedding_conv_kernel: 3
  use_hope: false
  use_moe: false
  ffn_hidden_size: 3072
  ffn_activation: gelu
training:
  batch_size: 8
  dataloader_workers: 2
  max_length: 512
  mlm_probability: 0.15
  max_samples: 20000000
  dataset_batch_size: 25000
  num_workers: 8
  cache_dir: "./temp_data/v2_dataset_cache"
  use_amp: false
  gradient_accumulation_steps: 4
  lr_embeddings: 1e-6
  lr_norms: 5e-6
  lr_ode: 1e-7
  lr_retnet: 5e-6
  lr_mamba: 2e-6
  lr_attention: 3e-6
  lr_other: 2e-6
  wd_embeddings: 0.01
  wd_norms: 0.001
  wd_ode: 0.01
  wd_retnet: 0.01
  wd_mamba: 0.01
  wd_attention: 0.01
  wd_other: 0.01
  betas_embeddings: [0.9, 0.95]
  betas_norms: [0.9, 0.95]
  betas_ode: [0.9, 0.95]
  betas_retnet: [0.9, 0.95]
  betas_mamba: [0.9, 0.95]
  betas_attention: [0.9, 0.95]
  betas_other: [0.9, 0.95]
  eps_embeddings: 1e-8
  eps_norms: 1e-8
  eps_ode: 1e-8
  eps_retnet: 1e-8
  eps_mamba: 1e-8
  eps_attention: 1e-8
  eps_other: 1e-8
  scheduler_total_steps: 10000
  scheduler_warmup_ratio: 0.1
  scheduler_type: cosine
  grad_clip_max_norm: 5.0
  inf_post_clip_threshold: 100.0
  max_nan_retries: 3
  checkpoint_every_n_steps: 500
  max_rolling_checkpoints: 3
  num_best_checkpoints: 2
  nan_check_interval: 10
  log_gradient_stats: true
  gradient_log_interval: 10
  use_galore: false
  galore_rank: 64
  galore_update_interval: 1
  galore_scale: 1.0
  galore_max_dim: 4096
